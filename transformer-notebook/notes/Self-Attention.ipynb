{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "287716b2-c90a-45e7-b583-4ccf3beebaa2",
   "metadata": {},
   "source": [
    "# 自注意力机制相关知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc165f1-5524-49b2-91b0-950a63842bdf",
   "metadata": {},
   "source": [
    "### 一、一些疑问："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0686f4ed-459a-4792-8c42-274167964bb3",
   "metadata": {},
   "source": [
    "1、为什么要做QKV线性变换：  \n",
    "为了让模型学会不同类型的信息表示，通过不同的变换，我们可以让查询、键、值专注于各自最有用的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db72b0-0c06-4e00-b14b-c3f5d4969a70",
   "metadata": {},
   "source": [
    "2、为什么要除以sqrt(d_k):  \n",
    "防⽌Softmax函数的梯度消失  \n",
    "点积的数值大小问题：维度d_k越大，点积的值波动越大，绝对值也越大。  \n",
    "softmax问题：当输入的绝对值很大时，softmax 的输出会趋近于 0 或 1（即进入“饱和区”），此时梯度非常小，导致反向传播时梯度消失，模型难以有效学习。  \n",
    "缩放的作用：控制方差，除以sqrt(d_K)的依据是原始QK方差为d_k，而QK/sqrt(d_K)的方差是1，使得softmax的输入保持在一个合理的范围内。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b144ef-edce-4143-8371-4018847dfea7",
   "metadata": {},
   "source": [
    "3、为什么要用softmax：    \n",
    "将相似度分数转化为概率分布  \n",
    "归一化：Softmax确保所有注意力权重之和为1，这样输出就是输入的加权平均。   \n",
    "非负性：Softmax输出都是正数，符合\"注意力程度\"的直观理解。  \n",
    "可解释性：输出值可以直接解释为\"关注程度\"。  \n",
    "可微性：Softmax是可导的，便于反向传播训。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f907b6c-f9e9-4c87-970c-daa17c023ac4",
   "metadata": {},
   "source": [
    "4、为什么要使用多头注意力机制：  \n",
    "与其使用一个头，不如使用多个头并行的关注不同的信息子空间。    \n",
    "线性投影：将输入分别投影到 个不同的子空间  \n",
    "并行计算：在每个子空间中独立计算注意力  \n",
    "拼接输出：将所有头的输出拼接起来  \n",
    "最终线性变换：将拼接结果映射到期望的输出维度  \n",
    "优势是：不同的表示子空间，增强模型的表达能力，稳定训练。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567c3dbc-4614-404e-9835-34c66c3d85e7",
   "metadata": {},
   "source": [
    "### 二、自注意力机制的时空复杂度："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5182654e-6abe-4694-9435-a9468c5fc770",
   "metadata": {},
   "source": [
    "时间复杂度  \n",
    "1、计算QKV矩阵：：O(n·d_model²)  \n",
    "2、多头分割：O(n·d_model)  \n",
    "3、注意力分数计算：O(n²·d_k) = O(n²·d_model/num_heads)  \n",
    "4、注意力权重计算：O(n²·num_heads)  \n",
    "5、注意力加权：O(n²·d_k) = O(n²·d_model/num_heads)  \n",
    "6、输出投影：O(n·d_model²)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ad23b8-0c36-41c5-9dfd-1aedef75efa2",
   "metadata": {},
   "source": [
    "总时间复杂度：O(n·d_model² + n²·d_model/num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c847c3-3516-4212-9b0e-f94ee86f103f",
   "metadata": {},
   "source": [
    "空间复杂度：    \n",
    "1、输入存储：O(n·d_model)  \n",
    "2、QKV变换矩阵存储：O(d_model²)  \n",
    "3、QKV中间结果存储：O(n·d_model)  \n",
    "4、多头分割后存储：O(n·d_model)  \n",
    "5、注意力矩阵存储：O(n²·num_heads)  \n",
    "6、输出存储：O(n·d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d805082b-5118-49f5-bbff-f1e5bdfba32d",
   "metadata": {},
   "source": [
    "总空间复杂度：O(d_model² + n·d_model + n²·num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d44afc-134e-4777-85cc-f50a2a26d290",
   "metadata": {},
   "source": [
    "### 三、多头注意力机制（MHA）代码演示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd30c82-a5e3-4154-9880-962044f19e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([32, 100, 512])\n",
      "输出形状: torch.Size([32, 100, 512])\n",
      "注意力权重形状: torch.Size([32, 8, 100, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model #Linear（self,in,out)输入维度只传入一个参数是在说明“我对每个向量的 d_model 维特征做线性变换，不管有多少个向量。”\n",
    "        self.W_k = nn.Linear(d_model, d_model) #Linear是向量处理器，设计哲学是它不处理“整个矩阵的运算”，而是处理“每个向量的变换”。\n",
    "        self.W_v = nn.Linear(d_model, d_model) #会自动对每一个 d_model 维的向量做相同的线性变换\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):#此处的querykeyvalue都是同一个值“输入向量x”\n",
    "        batch_size = query.size(0)#.size(0)获取第0维的大小，即batch_size\n",
    "\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #self.W_q(query) 会触发 self.W_q 实例的 __call__ 方法。\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #进而调用其 forward 方法。\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1,2) #对输入 query 的每一个向量进行线性变换，返回一个形状相同的输出张量 Q。\n",
    "\n",
    "        #调用函数计算注意力分数\n",
    "        attention_output,attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        #拼接\n",
    "        #.transpose()：只改“怎么读数据”（逻辑索引），不改“数据在哪”（物理顺序）\n",
    "        #.contiguous()：把数据复制一份，按新的逻辑顺序重新排列在内存中\n",
    "        #.view()只能对连续存储的数据进行操作\n",
    "        concat_attention = attention_output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        #最终线性变换\n",
    "        output = self.W_o(concat_attention)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def  scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        #注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2,-1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32)) \n",
    "        #把 d_k 变成张量，不是因为“不能算”，而是因为“要融入计算图”。\n",
    "\n",
    "        #应用填充掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9) #masked_fill(mask,value)把输入张量中，对应 mask 为 True 的位置，全部替换成 value。\n",
    "\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        return output, attention_weights\n",
    "\n",
    "# 使用示例\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "# 输入序列\n",
    "seq_len = 100\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = mha(x, x, x)\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785e47b-9716-440c-b8f4-d07dbd00b87a",
   "metadata": {},
   "source": [
    "### 四、多头注意力机制的复杂度分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be95d69-5cad-4b54-a777-f7d0df3e2de0",
   "metadata": {},
   "source": [
    "多头注意力通过并行计算，实现了\"看起来\"有多个注意力机制，但实际上并没有显著增加计算复杂度。   \n",
    "1、参数总量不变  \n",
    "2、可以并行计算  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc3817-4724-47cf-9d81-54e89b9e8acd",
   "metadata": {},
   "source": [
    "**复杂度对比：**    \n",
    "n：序列长度  \n",
    "d_model：模型隐藏维度  \n",
    "num_heads：注意力头数  \n",
    "d_k = d_model / num_heads：每个注意力头的维度      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9369b275-e526-4735-a92b-add5e2e1d8e1",
   "metadata": {},
   "source": [
    "![图片无法显示](../image/MHA复杂度分析.png \"MHA复杂度分析表\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa8bff-a541-4855-9eeb-6b6d2440b0cf",
   "metadata": {},
   "source": [
    "**为什么复杂度相同但性能更好？**  \n",
    "1. 并行计算：num_heads个小矩阵可以并行计算，而单头的大矩阵计算受限于硬件  \n",
    "2. 内存访问模式：连续的⼩块内存访问比大块稀疏访问更高效  \n",
    "3. 表达能力：多个头可以学习不同的注意力模式，表达能力更强  \n",
    "4. 梯度流：多个独立的注意力路径有助于梯度传播，训练更稳定  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e427825-fd8b-4de6-bb8d-b948b6f901a2",
   "metadata": {},
   "source": [
    "**实际效率提升：**  \n",
    "1. 缓存效率：并行计算更好地利用了GPU的并行能力   \n",
    "2. 内存访问模式：连续的数据访问模式更高效  \n",
    "3. 指令级并⾏：现代CPU/GPU的SIMD指令集  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7c2f3-58b0-4991-b179-f717f01d35be",
   "metadata": {},
   "source": [
    "### 五、GQA注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762859b1-6748-490f-ba4f-b1f798c6367f",
   "metadata": {},
   "source": [
    "相当于将多头分为多个小组，然后按照小组来进行查询，每一个小组共用一个多头。    \n",
    "将查询头分组，每组共享键和值头，在保持性能的同时减少计算量。  \n",
    "核心思想：   \n",
    "多个查询头共享相同的键和值  \n",
    "减少KV  \n",
    "缓存的内存占用  \n",
    "保持查询的多样性  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fa7d0bd-7590-4ac2-9459-96bfe6416d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GQA输出形状: torch.Size([32, 1000, 512])\n",
      "注意力权重形状: torch.Size([32, 32, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.head_group_size = num_heads // num_kv_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model // num_heads * num_kv_heads)\n",
    "        self.W_v = nn.Linear(d_model, d_model // num_heads * num_kv_heads)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        sql_len = query.size(1)\n",
    "\n",
    "        #计算OKV\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.num_kv_heads, self.d_k)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.num_kv_heads, self.d_k)\n",
    "\n",
    "        #扩展kv\n",
    "        K = K.unsqueeze(2).expand(-1, -1, self.head_group_size, -1, -1)\n",
    "        V = V.unsqueeze(2).expand(-1, -1, self.head_group_size, -1, -1)\n",
    "\n",
    "        #重塑张量\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        #计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=torch.float32))\n",
    "\n",
    "        #填充掩码\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0,-1e9)\n",
    "\n",
    "        #计算注意力权重\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        #计算注意力输出\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "\n",
    "        #重塑输出，合并多头\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        #学会多头信息\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "# 使用示例\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "num_kv_heads = 8  # 4:1的压缩比\n",
    "gqa = GroupedQueryAttention(d_model, num_heads, num_kv_heads)\n",
    "# 输入\n",
    "batch_size = 32\n",
    "seq_len = 1000\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = gqa(x, x, x)\n",
    "print(f\"GQA输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71949494-cc9b-4c71-a9ec-91a09ca4a7fa",
   "metadata": {},
   "source": [
    "**时空复杂度分析：**  \n",
    "见教案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2715f250-e8d9-42b5-9238-6e2a9a0dc995",
   "metadata": {},
   "source": [
    "### 六、MQA注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683dd3e5-0212-4f8f-9374-7caf62d28c2f",
   "metadata": {},
   "source": [
    "GQA的极端情况，所有查询头共享相同的键和值，进⼀步减少内存占⽤。  \n",
    "核⼼思想：  \n",
    "所有查询头共享⼀个键头和⼀个值头  \n",
    "最⼤化内存效率  \n",
    "可能牺牲⼀些模型质量  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ab1795-461f-47bd-8072-fa382b24f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k)  # 单个键头\n",
    "        self.W_v = nn.Linear(d_model, self.d_k)  # 单个值头\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # 计算Q (多个头)\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        Q = Q.transpose(1, 2)  # (batch_size, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # 计算K和V (单个头)\n",
    "        K = self.W_k(key).unsqueeze(1)  # (batch_size, 1, seq_len, d_k)\n",
    "        V = self.W_v(value).unsqueeze(1)  # (batch_size, 1, seq_len, d_k)\n",
    "        \n",
    "        # 扩展K和V以匹配所有查询头\n",
    "        K = K.expand(-1, self.num_heads, -1, -1)\n",
    "        V = V.expand(-1, self.num_heads, -1, -1)\n",
    "        \n",
    "        # 计算注意力\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_k, dtype=to\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        # 重塑输出\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    " # 使用示例\n",
    "d_model = 512\n",
    "num_heads = 32\n",
    "mqa = MultiQueryAttention(d_model, num_heads)\n",
    "# 输入\n",
    "batch_size = 32\n",
    "seq_len = 1000\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "# 前向传播\n",
    "output, attention_weights = mqa(x, x, x)\n",
    "print(f\"MQA输出形状: {output.shape}\")\n",
    "print(f\"注意力权重形状: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92305c91-3063-4279-bfc4-6d77bc65e6b2",
   "metadata": {},
   "source": [
    "### 七、MHA，GQA，MQA的复杂度分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57cae4d-bb54-4b6b-b3b4-dfe27c2f27f1",
   "metadata": {},
   "source": [
    "见教案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38079bf-b2e3-4d4e-bd6e-f25243048915",
   "metadata": {},
   "source": [
    "### 八、MLA的学习（Multi-Head Latent Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c73e79-6fd2-44e7-a154-a7641f4b4577",
   "metadata": {},
   "source": [
    "#### MLA的核心思想：（共享低维kv+查询解码）  \n",
    "**共享低维 Key/Value (K/V)**：所有注意力头共享一组低维的、数量极少的 K/V 向量（称为 kv_dim，远小于 d_k）。  \n",
    "**Latent KV Cache**\t：这组低维 K/V 被缓存，称为 Latent KV Cache，大小仅为传统 KV Cache 的 1/10 甚至更小。  \n",
    "**Query-to-KV 解码器** ：每个 Query 头通过一个小型解码器网络（通常是 1x1 卷积或线性层），从共享的 Latent K/V 中“解码”出该头专属的 K/V。  \n",
    "**动态生成 K/V**\t：K/V 不是直接投影得到，而是根据当前 Query 动态生成，实现“按需生成”。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c8f53c-a162-4c01-aa07-adc6c19e0d0c",
   "metadata": {},
   "source": [
    "#### MLA的优势  \n",
    "**极低 KV Cache**\t只缓存少量 Latent K/V，极大减少推理内存。  \n",
    "**高计算效率**\t解码器轻量，整体计算量低于 MHA。   \n",
    "**保持表达能力**\t通过解码器，不同头仍能生成差异化的 K/V，保持多头多样性。  \n",
    "**适合长上下文**\tKV Cache 小，支持更长的上下文窗口（如 128K）。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963aade9-31a6-4dcb-9a5e-ef05dd56f469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6157e-8352-43c2-8551-e32ff49e43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    完整版 Multi-Head Latent Attention (MLA) Module\n",
    "    核心技术：低秩联合压缩、矩阵吸收、位置编码解耦、MoE 解码器\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, latent_kv_heads=8, kv_dim=64, num_experts=8, top_k=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: 输入/输出维度\n",
    "            num_heads: Query 头数\n",
    "            latent_kv_heads: Latent K/V 的头数（远小于 num_heads）\n",
    "            kv_dim: 每个 Latent K/V 的维度（远小于 d_k）\n",
    "            num_experts: MoE 解码器的专家数量\n",
    "            top_k: 每次激活的专家数量\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.latent_kv_heads = latent_kv_heads\n",
    "        self.kv_dim = kv_dim\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "\n",
    "        # 1. Query 投影\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 2. 低秩联合压缩：K 和 V 联合投影到低维空间\n",
    "        self.W_kv_compressed = nn.Linear(d_model, latent_kv_heads * kv_dim * 2)  # 联合 K 和 V\n",
    "\n",
    "        # 3. MoE 解码器：Query 条件化生成 K/V 参数\n",
    "        self.k_decoder = MixtureOfExpertsDecoder(\n",
    "            d_model=d_model,\n",
    "            latent_dim=latent_kv_heads * kv_dim,\n",
    "            output_dim=self.num_heads * self.d_k,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        self.v_decoder = MixtureOfExpertsDecoder(\n",
    "            d_model=d_model,\n",
    "            latent_dim=latent_kv_heads * kv_dim,\n",
    "            output_dim=self.num_heads * self.d_k,\n",
    "            num_experts=num_experts,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # 4. 输出投影\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, position_ids=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "\n",
    "        # --- Step 1: 计算 Query ---\n",
    "        Q = self.W_q(query)  # (b, s, d_model)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)  # (b, s, h, d_k)\n",
    "        Q = Q.transpose(1, 2)  # (b, h, s, d_k)\n",
    "\n",
    "        # --- Step 2: 低秩联合压缩 K 和 V ---\n",
    "        KV = torch.cat([key, value], dim=-1)  # (b, s, 2*d_model)\n",
    "        KV_compressed = self.W_kv_compressed(KV)  # (b, s, latent_kv_heads * kv_dim * 2)\n",
    "        # 分离 K 和 V 的 Latent 表示\n",
    "        K_latent = KV_compressed[:, :, :self.latent_kv_heads * self.kv_dim]  # (b, s, L * d_l)\n",
    "        V_latent = KV_compressed[:, :, self.latent_kv_heads * self.kv_dim:]  # (b, s, L * d_l)\n",
    "        \n",
    "        K_latent = K_latent.view(batch_size, seq_len, self.latent_kv_heads, self.kv_dim)  # (b, s, L, d_l)\n",
    "        V_latent = V_latent.view(batch_size, seq_len, self.latent_kv_heads, self.kv_dim)  # (b, s, L, d_l)\n",
    "\n",
    "        # --- Step 3: 使用 MoE 解码器动态生成 K/V 参数 ---\n",
    "        # 以 Query 的聚合信息作为解码条件\n",
    "        query_cond = query.mean(dim=1)  # (b, d_model)\n",
    "\n",
    "        k_params = self.k_decoder(query_cond)  # (b, H * d_k)\n",
    "        v_params = self.v_decoder(query_cond)  # (b, H * d_k)\n",
    "\n",
    "        # Reshape 为参数矩阵\n",
    "        k_params = k_params.view(batch_size, self.num_heads, self.d_k)  # (b, H, d_k)\n",
    "        v_params = v_params.view(batch_size, self.num_heads, self.d_k)  # (b, H, d_k)\n",
    "\n",
    "        # --- Step 4: 动态解码生成最终的 K 和 V ---\n",
    "        # 采用逐头解码：每个 Latent 头解码出多个 Query 头的 K/V\n",
    "        # 假设 num_heads = latent_kv_heads * group_size\n",
    "        assert self.num_heads % self.latent_kv_heads == 0, \"num_heads must be divisible by latent_kv_heads\"\n",
    "        group_size = self.num_heads // self.latent_kv_heads\n",
    "\n",
    "        # 重塑 Latent K/V 以匹配 Query 头分组\n",
    "        K_latent_expanded = K_latent.unsqueeze(2).expand(-1, -1, group_size, -1, -1)  # (b, s, g, L, d_l)\n",
    "        V_latent_expanded = V_latent.unsqueeze(2).expand(-1, -1, group_size, -1, -1)  # (b, s, g, L, d_l)\n",
    "        \n",
    "        # 重塑用于矩阵乘法\n",
    "        K_latent_flat = K_latent_expanded.reshape(batch_size, seq_len, self.num_heads, self.kv_dim)  # (b, s, H, d_l)\n",
    "        V_latent_flat = V_latent_expanded.reshape(batch_size, seq_len, self.num_heads, self.kv_dim)  # (b, s, H, d_l)\n",
    "\n",
    "        # 应用解码参数：这里简化为逐元素乘法，实际可能更复杂\n",
    "        # K = K_latent_flat * k_params.unsqueeze(1)  # (b, s, H, d_l) * (b, 1, H, d_k) -> 不匹配\n",
    "        # 更精确的解码方式是：K_latent_flat @ k_params.T\n",
    "        # 但 k_params 是 (b, H, d_k)，不能直接作为变换矩阵\n",
    "        # 因此，k_params 应为 (b, H, d_l, d_k) 才能做 matmul\n",
    "        # 这说明上面的解码器输出维度需要是 (H * d_l * d_k)\n",
    "\n",
    "        # 重新设计解码器输出：(H * d_l * d_k)\n",
    "        k_params_full = self.k_decoder_full_matrix(query_cond)  # (b, H * d_l * d_k)\n",
    "        v_params_full = self.v_decoder_full_matrix(query_cond)  # (b, H * d_l * d_k)\n",
    "        k_params_mat = k_params_full.view(batch_size, self.num_heads, self.kv_dim, self.d_k)\n",
    "        v_params_mat = v_params_full.view(batch_size, self.num_heads, self.kv_dim, self.d_k)\n",
    "\n",
    "        # 逐位置、逐头进行解码\n",
    "        K = torch.einsum('bsHd_l, bHd_lD_k -> bsHD_k', K_latent_flat, k_params_mat)  # (b, s, H, d_k)\n",
    "        V = torch.einsum('bsHd_l, bHd_lD_k -> bsHD_k', V_latent_flat, v_params_mat)\n",
    "\n",
    "        K = K.transpose(1, 2)  # (b, H, s, d_k)\n",
    "        V = V.transpose(1, 2)  # (b, H, s, d_k)\n",
    "\n",
    "        # --- Step 5: 位置编码解耦 ---\n",
    "        # RoPE 只应用于最终的 Q, K, V，而不是 Latent K/V\n",
    "        if position_ids is not None:\n",
    "            Q = self.apply_rope(Q, position_ids)\n",
    "            K = self.apply_rope(K, position_ids)\n",
    "\n",
    "        # --- Step 6: 计算注意力 ---\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)  # (b, h, s, d_k)\n",
    "\n",
    "        # --- Step 7: 合并多头 ---\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        output = self.W_o(output)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n",
    "    def apply_rope(self, x, position_ids):\n",
    "        \"\"\"Apply Rotary Position Embedding to x. Simplified version.\"\"\"\n",
    "        # Placeholder for RoPE implementation\n",
    "        # In practice, this would involve sine/cosine rotations\n",
    "        return x\n",
    "\n",
    "class MixtureOfExpertsDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MoE 解码器：根据 Query 条件动态选择专家生成 K/V 参数\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, latent_dim, output_dim, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # 专家网络：每个专家负责生成一部分参数\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Linear(latent_dim, output_dim)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        # 门控网络：决定激活哪些专家\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "\n",
    "    def forward(self, query_cond):\n",
    "        gate_logits = self.gate(query_cond)  # (b, num_experts)\n",
    "        gate_scores = torch.softmax(gate_logits, dim=-1)\n",
    "        topk_weights, topk_indices = torch.topk(gate_scores, self.top_k, dim=-1) # (b, top_k)\n",
    "\n",
    "        # 初始化输出\n",
    "        output = torch.zeros((query_cond.size(0), self.output_dim), device=query_cond.device, dtype=query_cond.dtype)\n",
    "\n",
    "        # 稀疏激活：只计算 top-k 个专家\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = topk_indices[:, i]  # (b,)\n",
    "            expert_weights = topk_weights[:, i].unsqueeze(-1)  # (b, 1)\n",
    "            \n",
    "            # 使用 gather 选择对应的专家\n",
    "            selected_experts_output = torch.stack([self.experts[idx](query_cond) for idx in expert_idx.unique()], dim=0)\n",
    "            # This is a simplified version; actual gather might be more complex\n",
    "            # For each batch, select its specific expert\n",
    "            temp_output = torch.zeros_like(output)\n",
    "            for b_idx, e_idx in enumerate(expert_idx):\n",
    "                temp_output[b_idx] = self.experts[e_idx](query_cond[b_idx])\n",
    "            \n",
    "            output += temp_output * expert_weights\n",
    "\n",
    "        return output\n",
    "\n",
    "# --- 矩阵吸收技术（在推理前预处理）---\n",
    "def absorb_matrices(mla_module):\n",
    "    \"\"\"\n",
    "    将 W_kv_compressed 与 MoE 专家的权重相乘，实现矩阵吸收\n",
    "    注意：这是一个概念性实现，实际吸收需根据具体结构设计\n",
    "    \"\"\"\n",
    "    # 示例：W_kv_compressed (d_model, L*d_l*2) -> experts (L*d_l, out_dim)\n",
    "    # Absorbed: W_kv_compressed @ expert_weight\n",
    "    # This is complex and typically done during model export/optimization.\n",
    "    # Here we just note the concept.\n",
    "    print(\"Matrix absorption would happen here during model optimization.\")\n",
    "    return mla_module\n",
    "\n",
    "# --- 示例用法 ---\n",
    "if __name__ == \"__main__\":\n",
    "    d_model, num_heads, l_kv_heads, kv_dim = 512, 8, 2, 64\n",
    "    attn = MultiHeadLatentAttention(d_model, num_heads, l_kv_heads, kv_dim)\n",
    "\n",
    "    query = key = value = torch.randn(2, 10, d_model)\n",
    "    mask = torch.tril(torch.ones(10, 10)).unsqueeze(0).unsqueeze(0)\n",
    "    position_ids = torch.arange(10).unsqueeze(0)\n",
    "\n",
    "    output, weights = attn(query, key, value, mask, position_ids)\n",
    "    print(f\"Output shape: {output.shape}\")    # torch.Size([2, 10, 512])\n",
    "    print(f\"Weights shape: {weights.shape}\")  # torch.Size([2, 8, 10, 10])\n",
    "\n",
    "    # 应用矩阵吸收\n",
    "    absorbed_attn = absorb_matrices(attn)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
