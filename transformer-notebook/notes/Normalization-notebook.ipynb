{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8330d13e-721c-4a2b-adfc-5dce6729ac79",
   "metadata": {},
   "source": [
    "# 一、为什么大模型需要归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875166b-6220-4d83-8ddc-500090e2bbc2",
   "metadata": {},
   "source": [
    "## 1、协变量偏移问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0c045-5d11-41af-96db-b334a51ae184",
   "metadata": {},
   "source": [
    "协变量偏移指的是在训练过程中，每一层的输入分布发生变化的现象。在深度神经网络中，前面层的参数更新，会影响后面层的输入分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d086a86b-5aca-4634-ade1-6f254eb8a6d4",
   "metadata": {},
   "source": [
    "## 2、归一化的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e147504-1257-4384-b543-b8ed33e0a314",
   "metadata": {},
   "source": [
    "它的核心作用是稳定训练过程：  \n",
    "1、减少内部协变量偏移：通过固定每层输入的分布，让网络更容易学习。  \n",
    "2、允许使用更大的学习率：归一化后的网络对超参数不那么敏感。  \n",
    "3、减少对初始化的依赖：即使初始化不太好，归一化也可以帮助正常训练。  \n",
    "4、有一定的正则化效果：可以减少过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c9d99-7c3d-4d48-aa89-01adb3c4a2f7",
   "metadata": {},
   "source": [
    "# 二、批归一化BN（BatchNormalizaton):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e45619-3aae-41be-944b-0b6f0f7c4013",
   "metadata": {},
   "source": [
    "BN的核心思想：通过规范化每一层的输入，使其分布保持稳定，从而缓解内部协变量偏移。  \n",
    "他期望的输入是图像或者全连接，不用于transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1be9f-3e07-439e-8651-e98af3f6ea1b",
   "metadata": {},
   "source": [
    "### 工作原理：  \n",
    "假设我们有一个神经网络层，其输入为一个 mini-batch 的数据$X={x_1,x_2,x_3,...x_m}$,其中m是batch_szie,  \n",
    "BatchNorm 对每个特征维度（卷积里面是 channel 维度）独立进行归一化。我们以一个特征维度为例，说明其计算过程：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d794480-0f15-4d28-af75-014fac66b453",
   "metadata": {},
   "source": [
    "步骤一：计算mini_batch的均值和方差：  \n",
    "$$\n",
    "\\mu_B=\\frac{1}{m}\\sum_{i=1}^m x_i\n",
    "$$\n",
    "$$\n",
    "\\sigma_B^2=\\frac{1}{m}\\sum_{i=1}^m (x_i-\\mu_B)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db203c4b-8af1-4051-95a1-dd73b1c771a1",
   "metadata": {},
   "source": [
    "步骤二：归一化  \n",
    "将每个输入减去均值、除以标准差，使其均值为 0、方差为 1，其中$\\epsilon$>0是一个极小的常数，用于防止除以0.\n",
    "$$\n",
    "\\hat{x}_i=\\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2+\\epsilon}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ba40d-f24b-4ea3-8a9e-ed99a58065f8",
   "metadata": {},
   "source": [
    "步骤三：缩放和平移（可学习参数）  \n",
    "如果只是归一化到均值 0、方差 1，可能会限制网络的表达能力。因此，BatchNorm 引入了两个通过反向传播学习的参数：\n",
    "γ（缩放参数）\n",
    "β（平移参数）\n",
    "$$y_i=\\gamma\\hat{x}_i+\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8c013-b343-4947-892e-c800797009f0",
   "metadata": {},
   "source": [
    "### BN训练和测试的区别："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2f394-6c5a-4a9a-9ab3-4f8fe521c669",
   "metadata": {},
   "source": [
    "训练时：  \n",
    "使用当前批次的均值和方差  \n",
    "同时更新移动平均的均值和方差  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c258a6-38d2-45d2-aab1-d1f62a6b3d65",
   "metadata": {},
   "source": [
    "测试时：  \n",
    "使用移动平均的均值和方差  \n",
    "不再计算批次统计量  \n",
    "保证测试的确定性  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c65640-b2ca-4b31-b177-9d637d3d7903",
   "metadata": {},
   "source": [
    "为什么需要这样设计？\n",
    "1. 训练稳定性：批次统计量有噪声，有助于正则化\n",
    "2. 测试确定性：测试时需要一致的结果\n",
    "3. 小批次问题：测试时可能批次很小，统计量不准确"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90603a-fb80-4606-b612-fcbf51ae8aab",
   "metadata": {},
   "source": [
    "### BN中的缩放因子与偏移因子："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d125f3c-9c0c-4b3d-b0b0-97cdd12ea571",
   "metadata": {},
   "source": [
    "主要作用：\n",
    "1. 恢复表达能力：纯归一化会损失一些表达能力\n",
    "2. 允许恒等映射：网络可以学会不进行归一化\n",
    "3. 适应不同分布：不同层可能需要不同的分布"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ded27f-3bde-41c3-b9d3-fcb5f4f2fa30",
   "metadata": {},
   "source": [
    "这两个因子可以在归一化后：  \n",
    "1.近似恢复原始分布。  \n",
    "2.变成不同的分布。  \n",
    "3.完全不变，恒等映射。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3018a5-2068-4fbf-bf4a-efcd655cb272",
   "metadata": {},
   "source": [
    "如果没有这两个因子，BN会强制所有层的输入都是标准正态分布，这可能会：  \n",
    "损失网络的表达能力   \n",
    "让某些层难以学习  \n",
    "阻止梯度流动（在某些情况下）  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410ff5d-6d05-470b-a1b3-3b34a50ae7fe",
   "metadata": {},
   "source": [
    "### BN减少过拟合的作用：  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471bedaa-5c43-41f3-8787-df1adde03e04",
   "metadata": {},
   "source": [
    "BN有一定的正则化的效果，能在一定程度上减少过拟合。  \n",
    "1、噪声引入：每个批次的均值和方差都不同，这种噪声类似于Dropout的效果，让网络更加鲁棒。  \n",
    "2、减少层间依赖：每层不再强烈依赖前一层的特定输出，让网络更加灵活。  \n",
    "3、稳定的梯度：稳定的梯度让网络更容易找到好的局部最优，减少陷入过拟合的风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33428733-0cfa-48a7-8260-4538d20d6511",
   "metadata": {},
   "source": [
    "### BN在NLP任务中的局限性："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759eb887-ff08-481a-bb27-57cc12a2b523",
   "metadata": {},
   "source": [
    "1、序列长度不一致：长短句不一，短句需要padding.  \n",
    "2、批次大小问题：NLP中批次较小，小批次统计量不稳定，批次均值和方差不准确。  \n",
    "3、时间序列特性：词序列有着明确的时间依赖关系，BN可能破坏这种关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef104d-b089-4000-a548-4cd2a2dd2e5a",
   "metadata": {},
   "source": [
    "# 三、LN层归一化（Layer Normalization）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce204d3-48c7-4dbf-aabc-581a6a37a31c",
   "metadata": {},
   "source": [
    "核心思想：对每个样本的每一层输出，在特征维度上进行归一化。单个样本的所有特征进行归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb17b9f-a7db-4a36-85a8-4712085f911f",
   "metadata": {},
   "source": [
    "### 工作原理："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5a0bd8-f764-4f70-89c1-f855f1241711",
   "metadata": {},
   "source": [
    "假设我们有一个batch的输入X维度是（batch_size, d_model),LN对每一个样本$i\\in[1,N]$独立进行归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386412bc-98fb-49cc-9fd8-2eb5bda1ba52",
   "metadata": {},
   "source": [
    "步骤一：计算该样本的所有均值和方差  \n",
    "对于第i个样本，$X_i=[x_{i1},x_{i2},x_{i3},...x_{id\\_model}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc157250-18fe-42a5-9202-4f0bb06f2a1d",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mu_i=\\frac{1}{d\\_model} \\sum_{j=1}^{d\\_model}x_{ij}\n",
    "$$\n",
    "$$\n",
    "\\sigma_i^2=\\frac{1}{d\\_model}\\sum_{j=1}^{d\\_model}(x_{ij}-\\mu_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc2842-96c7-4633-aa87-7db99eef90a4",
   "metadata": {},
   "source": [
    "步骤二：归一化\n",
    "$$\n",
    "\\hat{x}_{ij}=\\frac{x_{ij}-\\mu_i}{\\sqrt{\\sigma_i^2+\\epsilon}}\n",
    "$$\n",
    "其中$\\epsilon>0$是一个小常数，防止除0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c0f1d-64cb-446d-9f7e-fea935e807e4",
   "metadata": {},
   "source": [
    "步骤三：可反向传播优化的平移和缩放  \n",
    "$$\n",
    "y_{ij}=\\gamma_j \\hat{x_{ij}}+\\beta_j\n",
    "$$\n",
    "$\\gamma_j$缩放参数（scale），每个特征有一个  \n",
    "$\\beta_j$平移参数（shift），每个特征有一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150c7d5-89b4-4e67-ba45-77cf3758b6d4",
   "metadata": {},
   "source": [
    "**BN和LN的关键区别**：  \n",
    "BN：跨样本，归⼀化每个特征  \n",
    "LN：跨特征，归⼀化每个样本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6297297d-d2af-4077-a20f-a44d38aee82b",
   "metadata": {},
   "source": [
    "### LN在transformer的具体作用：  \n",
    "**稳定训练**：让每⼀层的输入分布保持稳定不受批次大小和序列长度的影响  \n",
    "**处理变长序列**：每个样本独立归⼀化不受padding和其他样本影响  \n",
    "**梯度流动**：帮助梯度更稳定地流动防止梯度消失或爆炸  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f0e74-e3b2-4401-b422-cc78f4fa70ef",
   "metadata": {},
   "source": [
    "为什么Transformer选择LN而不是BN？\n",
    "1. NLP的序列特性更适合样本内归⼀化\n",
    "2. 批次大小通常较小，BN效果不佳\n",
    "3. 变长序列问题得到自然解决\n",
    "4. 训练更加稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3feef9-8890-481d-88d1-a5ea5aa2216a",
   "metadata": {},
   "source": [
    "### 为什么LN能缓解梯度消失和爆炸："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029ae4a5-c6c0-45b8-a80f-f09c19433af9",
   "metadata": {},
   "source": [
    "1、稳定的输入分布：每层输入的均值和方差保持稳定，让参数更新更加可预测。  \n",
    "2、梯度缩放效应：LN的梯度有一定的自稳定特性  \n",
    "如果输入很大 → 归⼀化后变小 → 梯度不会爆炸  \n",
    "如果输入很小 → 归⼀化后变大 → 梯度不会消失  \n",
    "3、参数尺度不变性：LN让网络对参数尺度不那么敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d14574-6f3a-45de-9ff7-5abc7d7db093",
   "metadata": {},
   "source": [
    "### LN与BN的系统图表对比：    \n",
    "见教案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e0cd3-e389-4141-93fa-33702b36445e",
   "metadata": {},
   "source": [
    "# 四、RMSNorm现代优化方案"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577daa8f-1530-4a6e-8f37-ea1fb88fc240",
   "metadata": {},
   "source": [
    "RMSNorm是LN的一个简化变体。  \n",
    "核心思想：去掉均值计算，只归一化RMS（均方根）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5df0707-ae09-40ed-a33d-d7cd90eef52f",
   "metadata": {},
   "source": [
    "### 计算步骤： \n",
    "d一般指d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbc8415-50fa-45cb-a9c7-db6ef1db32c7",
   "metadata": {},
   "source": [
    "步骤一：计算RMS\n",
    "$$\n",
    "RMS(x)=\\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67bda50-8e7e-4f4e-b2b4-f54158241ab0",
   "metadata": {},
   "source": [
    "步骤二：归一化  \n",
    "$$\n",
    "\\hat{x_i}=\\frac{x_i}{RMS(x)+\\epsilon}\n",
    "$$\n",
    "步骤三：缩放\n",
    "$$\n",
    "y_i=\\gamma \\hat{x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c1762-7e49-4909-a8fc-cbf95af7cfe8",
   "metadata": {},
   "source": [
    "直观理解：RMSNorm假设数据的均值接近0，所以直接去掉了均值计算。这在很多情况下是合理的，特别是：  \n",
    "经过激活函数后（如ReLU，输出≥0）  \n",
    "某些特定的网络架构中大模型的隐藏层输出  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640c77f-7f8b-4186-b1da-cf66c16227df",
   "metadata": {},
   "source": [
    "### RMSNorm比LN的优势"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0974fc-6fe7-49ae-b34c-d1832df24ba2",
   "metadata": {},
   "source": [
    "1、计算量更少：  \n",
    "LN：需要计算均值和方差（2次遍历）  \n",
    "RMSNorm：只需要计算RMS（1次遍历）  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150f3947-3985-4eea-bcc0-449323e63ac6",
   "metadata": {},
   "source": [
    "2、数值稳定性：  \n",
    "避免了均值计算中的数值问题  \n",
    "在某些情况下更加稳定  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a5353f-778e-4e70-9629-9827c8fb293e",
   "metadata": {},
   "source": [
    "为什么能工作？  \n",
    "中心极限定理：大模型的隐藏层输出通常接近正态分布  \n",
    "激活函数：ReLU等激活函数的输出特性  \n",
    "残差连接：帮助保持数据的统计特性  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c133a-350b-4f95-a4f7-d38769a3a1df",
   "metadata": {},
   "source": [
    "# 五、pre-LN和post-LN的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31acacaa-d50e-4ab0-a697-8343d9efc197",
   "metadata": {},
   "source": [
    "### pre-LN：  \n",
    "LN在子层（注意力或FFN）**之前**，残差连接连接的是归⼀化后的输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34781e-8f0b-4909-b69c-f28ea9d87572",
   "metadata": {},
   "source": [
    "### post-LN：  \n",
    "LN在子层（注意力或FFN）之后  \n",
    "残差连接连接的是子层的输出  \n",
    "归一化在残差连接后"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c8f9e-bfd2-4ff2-bc07-4d9cbe50c7c0",
   "metadata": {},
   "source": [
    "### 两种架构的详细对比：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75010929-c5aa-46ed-8ba9-46bd92ae0a63",
   "metadata": {},
   "source": [
    "1、梯度流动：    \n",
    "pre-LN:梯度流动更直接、更稳定   \n",
    "post-LN:梯度在反向传播时必须经过 LayerNorm 层    \n",
    "2、反向传播：  \n",
    "pre-LN:梯度中有一个**恒等连接**（1），确保梯度可以无损回传    \n",
    "post-LN:梯度必须经过norm层，没有直接的恒等连接    \n",
    "3、训练稳定性：  \n",
    "pre-LN:梯度更稳定,对初始化不那么敏感,可以使用更大的学习率  \n",
    "post-LN:深层网络容易出现梯度问题，需要更仔细的初始化，学习率通常需要更小  \n",
    "4、收敛速度：  \n",
    "pre-LN:收敛更快（通常快20-30%）,训练曲线更平滑,更容易预测训练时间    \n",
    "post-LN:收敛较慢,训练曲线可能有波动,需要更多的调参工作  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3e2329-1500-44c5-857b-6b2170a05ba0",
   "metadata": {},
   "source": [
    "## 为什么现代大模型都采用pre-LN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b7157-b194-4e2d-a98b-09b2da741b53",
   "metadata": {},
   "source": [
    "1、训练稳定性：    \n",
    "100亿+参数的模型，训练成本极高  \n",
    "训练失败是无法接受的  \n",
    "Pre-LN提供了必要的稳定性   \n",
    "2、可扩展性：  \n",
    "Pre-LN可以很容易地扩展到更大模型，不需要重新调参，架构保持一致  \n",
    "3、工程实现：  \n",
    "实现更简单。调试更容易。监控指标更稳定  \n",
    "4、实际性能：  \n",
    "在大模型上，pre-LN的性能差距消失，有时甚至更好，训练效率更高"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
