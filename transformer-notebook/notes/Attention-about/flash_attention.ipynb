{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bda0df3-ea0c-42c2-b990-e7a76a025808",
   "metadata": {},
   "source": [
    "## FlashAttention的核心原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5a37d7-fc9d-43d3-b8b4-497393c26deb",
   "metadata": {},
   "source": [
    "1、内存高效的注意力计算：  \n",
    "FlashAttention的核⼼思想是避免存储巨⼤的注意⼒矩阵，⽽是通过分块计算来降低内存需求。   \n",
    "分块：不一次性计算整个注意力矩阵，而是将Q, K, V矩阵分成小块。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343bb38-0a1a-441b-b3f7-9295116ae6b3",
   "metadata": {},
   "source": [
    "2、平铺(Tiling)策略，适合高速缓存的IO优化:    \n",
    "平铺是FlashAttention的核心技术，它专门设计来解决IO性能瓶颈问题。    \n",
    "将输入序列分成小块（tiles），每个块大小适配⾼速缓存容量确保每个数据块都能放入GPU的共享内存（shared memory）  \n",
    "关键目标：最大化数据复用，最小化显存IO  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2cff6a-2c40-49d1-99f2-b323440b38fb",
   "metadata": {},
   "source": [
    "现代GPU的高速缓存容量：  \n",
    "L1缓存：128KB，延迟1ns，带宽50TB/s  \n",
    "共享内存：164KB，延迟1ns，带宽100TB/s  \n",
    "L2缓存：6MB，延迟3ns，带宽20TB/s  \n",
    "**三者关键区别：共享内存是唯一可以由程序员直接控制的高速片上内存。L1 和 L2 都是硬件自动管理的缓存。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49804e73-9f76-4b80-b336-ac0e846773e5",
   "metadata": {},
   "source": [
    "在 Flash Attention 中，我们选择一个较小的计算块大小（如 BLOCK_M=64, BLOCK_N=64），确保这个块所需的数据（Q块、K块、中间结果）能够完全放入 GPU 的共享内存（通常每 SM 有 32KB~96KB）。目标是在共享内存容量限制内，尽可能选择较大的块以最大化数据重用和计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fad1faf-0719-4bdb-b389-dc779d1fa848",
   "metadata": {},
   "source": [
    "3、重计算技巧（Recomputation）：  \n",
    "核心思想：在反向传播时，不存储中间激活值，而是重新计算它们。  \n",
    " 在前向传播时：  \n",
    "1、计算注意力分数  \n",
    "2、立即应用softmax  \n",
    "3、乘以值矩阵得到输出4. 丢弃注意力分数（不存储）      \n",
    "在反向传播时：  \n",
    "1、重新计算注意力分数     \n",
    "2、使用这些分数计算梯度    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03019fb4-0772-4f66-98a4-85531643ffbe",
   "metadata": {},
   "source": [
    "**前向传播优化：**  \n",
    "Flash Attention 前向通过分块 + 在线 Softmax，避免了 O(N²) 中间矩阵的存储，并利用共享内存加速计算，实现了 O(N) 显存 和 更高的计算效率。  \n",
    "其中最重要的为：**online softmax**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851803a2-b7d5-436d-a3e5-ab0e755e228f",
   "metadata": {},
   "source": [
    "### FlashAttention的简化代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c70ad90-1233-4dda-81a5-206c72ce2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def flash_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    简化的 Flash Attention 前向传播实现。\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor, shape [batch, heads, seq_len_q, head_dim]\n",
    "        K: Key tensor,   shape [batch, heads, seq_len_k, head_dim]\n",
    "        V: Value tensor, shape [batch, heads, seq_len_k, head_dim]\n",
    "        mask: Optional attention mask, shape [seq_len_q, seq_len_k]\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor, shape [batch, heads, seq_len_q, head_dim]\n",
    "    \"\"\"\n",
    "    batch, heads, seq_len_q, head_dim = Q.shape\n",
    "    seq_len_k = K.shape[2]\n",
    "    \n",
    "    # 确保 head_dim 不太大，避免数值问题\n",
    "    scaling = 1.0 / math.sqrt(head_dim)\n",
    "    \n",
    "    # 初始化输出、归一化参数\n",
    "    # 这些是 O(N) 的，而不是 O(N²)\n",
    "    O = torch.zeros_like(Q)  # 输出，逐块累加\n",
    "    l = torch.zeros(batch, heads, seq_len_q, device=Q.device)  # 累积和 (log-sum-exp 的分母部分)\n",
    "    m = torch.full((batch, heads, seq_len_q), -float('inf'), device=Q.device)  # 当前最大值\n",
    "    \n",
    "    # 定义块大小 (Tile Size)\n",
    "    # 这是关键的优化参数！\n",
    "    # 在真实实现中，会根据 GPU 架构自动调优\n",
    "    BLOCK_Q = 64  # Q 的块大小（按行）\n",
    "    BLOCK_K = 64  # K, V 的块大小（按列）\n",
    "    \n",
    "    # 分块处理：遍历 Q 的行块\n",
    "    for q_start in range(0, seq_len_q, BLOCK_Q):\n",
    "        q_end = min(q_start + BLOCK_Q, seq_len_q)\n",
    "        Q_block = Q[:, :, q_start:q_end]  # [b, h, block_q, d]\n",
    "        \n",
    "        # 初始化当前 Q 块的归一化参数\n",
    "        l_block = l[:, :, q_start:q_end]  # [b, h, block_q]\n",
    "        m_block = m[:, :, q_start:q_end]  # [b, h, block_q]\n",
    "        \n",
    "        # 遍历 K, V 的列块\n",
    "        for k_start in range(0, seq_len_k, BLOCK_K):\n",
    "            k_end = min(k_start + BLOCK_K, seq_len_k)\n",
    "            K_block = K[:, :, k_start:k_end]  # [b, h, block_k, d]\n",
    "            V_block = V[:, :, k_start:k_end]  # [b, h, block_k, d]\n",
    "            \n",
    "            # Step 1: 计算 Q_block @ K_block^T\n",
    "            # 这个矩阵大小为 [b, h, block_q, block_k]\n",
    "            # 在真实实现中，这一步在共享内存中进行\n",
    "            S_block = torch.matmul(Q_block, K_block.transpose(-2, -1)) * scaling  # [b, h, q_block, k_block]\n",
    "            \n",
    "            # 如果有 mask，应用 mask\n",
    "            if mask is not None:\n",
    "                mask_block = mask[q_start:q_end, k_start:k_end]\n",
    "                S_block = S_block.masked_fill(mask_block == 0, float('-inf'))\n",
    "            \n",
    "            # Step 2: 在线 Softmax 的核心 - 更新归一化参数\n",
    "            # 2.1 找到当前块的最大值\n",
    "            block_max = torch.max(S_block, dim=-1, keepdim=True)[0]  # [b, h, block_q, 1]\n",
    "            \n",
    "            # 2.2 更新全局最大值: new_max = max(m_block, block_max)\n",
    "            new_max = torch.maximum(m_block.unsqueeze(-1), block_max)  # [b, h, block_q, 1]\n",
    "            \n",
    "            # 2.3 更新累积和 s\n",
    "            # 先将历史累积和 \"平移\" 到新的最大值下\n",
    "            # exp(m_block - new_max) 是一个缩放因子\n",
    "            exp_scaled_l = l_block.unsqueeze(-1) * torch.exp(m_block.unsqueeze(-1) - new_max)  # [b, h, block_q, 1]\n",
    "            \n",
    "            # 计算当前块的指数和\n",
    "            exp_S = torch.exp(S_block - new_max)  # [b, h, block_q, k_block]\n",
    "            block_sum = torch.sum(exp_S, dim=-1, keepdim=True)  # [b, h, block_q, 1]\n",
    "            \n",
    "            # 更新累积和: s_new = s_old * exp(m_old - m_new) + sum(exp(S_new - m_new))\n",
    "            l_block_new = exp_scaled_l + block_sum  # [b, h, block_q, 1]\n",
    "            l_block_new = l_block_new.squeeze(-1)  # [b, h, block_q]\n",
    "            \n",
    "            # 2.4 更新最大值\n",
    "            m_block_new = new_max.squeeze(-1)  # [b, h, block_q]\n",
    "            \n",
    "            # Step 3: 计算当前块对输出的贡献\n",
    "            # 使用新的归一化参数计算 softmax 并乘以 V_block\n",
    "            # P_block = exp(S_block - new_max) / l_block_new\n",
    "            P_block = exp_S / (l_block_new.unsqueeze(-1) + 1e-6)  # 防止除零 [b, h, q_block, k_block]\n",
    "            O_block_contribution = torch.matmul(P_block, V_block)  # [b, h, q_block, d]\n",
    "            \n",
    "            # Step 4: 累加到输出\n",
    "            # 注意：输出是累加的，因为 softmax 是分块归一化的\n",
    "            O[:, :, q_start:q_end] = O[:, :, q_start:q_end] + O_block_contribution\n",
    "            \n",
    "            # Step 5: 更新归一化参数，用于下一个 K 块\n",
    "            l[:, :, q_start:q_end] = l_block_new\n",
    "            m[:, :, q_start:q_end] = m_block_new\n",
    "        \n",
    "        # 结束 K 块循环\n",
    "    # 结束 Q 块循环\n",
    "    \n",
    "    return O\n",
    "\n",
    "\n",
    "# ==================== 使用示例 ====================\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 模拟输入\n",
    "    batch = 2\n",
    "    heads = 8\n",
    "    seq_len = 128  # 可以尝试更大的值，如 1024\n",
    "    head_dim = 64\n",
    "    dtype = torch.float16  # 使用 float16 更接近真实场景\n",
    "    \n",
    "    Q = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device='cuda')\n",
    "    K = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device='cuda')\n",
    "    V = torch.randn(batch, heads, seq_len, head_dim, dtype=dtype, device='cuda')\n",
    "    \n",
    "    # 创建一个简单的 mask（可选）\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device='cuda'))\n",
    "    \n",
    "    # 计算 Flash Attention\n",
    "    with torch.no_grad():\n",
    "        output = flash_attention(Q, K, V, mask)\n",
    "    \n",
    "    print(f\"输入形状: Q={Q.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "    print(f\"计算完成！峰值显存占用远低于 O(N²)。\")\n",
    "    \n",
    "    # 对比：传统 Attention 的显存占用是 O(N²)\n",
    "    # 例如，seq_len=1024 时，S 矩阵需要 1024*1024*2 (float16) ≈ 2MB per head\n",
    "    # 而 Flash Attention 只需要 O(N) 的额外存储（l 和 m 向量）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
