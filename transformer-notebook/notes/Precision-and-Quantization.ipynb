{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4343fa72-cfac-4240-9b19-e1f34c3bdac6",
   "metadata": {},
   "source": [
    "# 一、精度相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13175e07-6aa4-4691-a276-d8fdebdbba82",
   "metadata": {},
   "source": [
    "**精度定义：**  \n",
    "在计算机中，数值精度指的是表示数字时所使用的位数和格式。精度越高，能够表示的数值范围越大，表示的细节越精确，但同时占用的存储空间也越大，计算复杂度也越高。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f66d0a-bc5e-42ef-a78f-b74cfccbfbe6",
   "metadata": {},
   "source": [
    "**常见精度类型：**  \n",
    "1、FP32(32位单精度浮点数)：    \n",
    "结构：1位符号位 + 8位指数位 + 23位尾数位  \n",
    "数值范围：约±3.4×10^38  \n",
    "精度：约7位十进制有效数字  \n",
    "存储需求：4字节  \n",
    "特点：精度高，但存储和计算开销大    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14380a4-6855-4653-a590-ba0b29e82427",
   "metadata": {},
   "source": [
    "2、FP16(16位半精度浮点数)：    \n",
    "结构：1位符号位 + 5位指数位 + 10位尾数位  \n",
    "数值范围：约±6.5×10^4  \n",
    "精度：约3-4位⼗进制有效数字  \n",
    "存储需求：2字节  \n",
    "特点：存储和计算效率高，但精度和数值范围有限"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3d3951-5f40-40b3-8046-c68801193e75",
   "metadata": {},
   "source": [
    "3、BF16(Brain Floating Point):    \n",
    "结构：1位符号位 + 8位指数位 + 7位尾数位  \n",
    "数值范围：与FP32相同，约±3.4×10^38  \n",
    "精度：低于FP32，约2-3位十进制有效数字  \n",
    "存储需求：2字节  \n",
    "特点：保持了FP32的数值范围，但牺牲了精度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9329ca-3516-41cf-86b1-db1da77fd9ac",
   "metadata": {},
   "source": [
    "4、INT8(8位整数):  \n",
    "数值范围：-128到127（有符号）或0到255（无符号）  \n",
    "精度：整数精度  \n",
    "存储需求：1字节  \n",
    "特点：存储效率高，计算速度快，但表示范围有限  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31430c23-23ea-43da-ba7e-b122d3342e3b",
   "metadata": {},
   "source": [
    "5、INT4(4位整数):  \n",
    "数值范围：-8到7（有符号）或0到15（无符号）  \n",
    "精度：整数精度  \n",
    "存储需求：0.5字节  \n",
    "特点：极高的存储效率，但表示范围非常有限  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405940d2-aee0-4642-9e7f-44c946140261",
   "metadata": {},
   "source": [
    "**为什么BF16更适合训练？**  \n",
    "1、更大的数值范围：BF16具有与FP32相同的8位指数位，能够表示更大的数值范围，这对于训练过程中的梯度计算很重要。  \n",
    "2、数值稳定性，避免梯度消失/爆炸：在训练深度神经网络时，梯度值可能变化很大，BF16的大数值范围有助于保持数值稳定性（即梯度值不容易超出表示范围上限）。  \n",
    "3、训练兼容性：许多现代AI框架和硬件对BF16训练有良好的支持。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fdab85-c894-40a4-9500-f08ec476c68e",
   "metadata": {},
   "source": [
    "**为什么FP16更适合推理？**  \n",
    "1、更高的精度：FP16有10位尾数位，相比BF16的7位尾数位具有更高的精度。  \n",
    "2、硬件优化：许多移动设备和专用AI芯片对FP16有很好的硬件支持，计算效率高。  \n",
    "3、内存效率：FP16只需要FP32一半的存储空间，有助于减少模型大小。  \n",
    "4、推理稳定性：在推理过程中，数值范围通常不会像训练过程中那样极端，FP16的精度通常足够。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e16021-3dc7-49b5-9e2e-6acb91966afb",
   "metadata": {},
   "source": [
    "# 二、量化相关概念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f36ef5-d8db-43a8-a5ae-b22ebe1f1a5a",
   "metadata": {},
   "source": [
    "**量化定义：**  \n",
    "量化是将高精度数值（如FP32）映射到低精度数值（如INT8或INT4）的过程。在大语言模型中，量化技术可以显著减少模型的存储需求和计算复杂度。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd375b9-d9fd-4a71-8e6c-6bfafe820b15",
   "metadata": {},
   "source": [
    "**量化的作用：**  \n",
    "1、减少存储需求：将FP32（4字节）量化为INT8（1字节）可以减少75%的存储需求。  \n",
    "2、提高计算效率：整数运算比浮点运算更快，特别是在专用硬件上。  \n",
    "3、降低功耗：减少内存访问和计算复杂度可以降低功耗。  \n",
    "4、便于部署：小模型更容易部署到资源受限的设备上。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e6c4ff-306f-481b-92f5-89a59306f743",
   "metadata": {},
   "source": [
    "**量化公式原理：**  \n",
    "$$\n",
    "Q(x)=round(\\frac{x}{s}+z)\n",
    "$$  \n",
    "x是原始高精度数值  \n",
    "Q(x)是量化后的低精度数值  \n",
    "s是缩放因子（scale）   \n",
    "z是零点偏移（zero point）  \n",
    "round是四舍五入函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2cd7ca-eac2-4eea-a27b-e62a74167ff6",
   "metadata": {},
   "source": [
    "**线性量化：**  \n",
    "是最常见的量化方法，可以将浮点数范围$[min,max]$映射到整数范围$[q_{min},q_{max}]$。  \n",
    "量化过程：  \n",
    "1、计算缩放因子：$s=\\frac{max-min}{q_{max}-q_{min}}$（缩放因子用于调整浮点数范围与整数表示范围之间的比例关系，它通常是一个正数。）  \n",
    "2、计算零点偏移：$z=q_{min}-round(\\frac{min}{s})$  \n",
    "（零点偏移的作用是通过调整量化映射的偏移量，确保浮点数中的零（0.0）能被精确表示为整数零点，从而保持数据稀疏性、减少量化误差并保证计算的数学一致性。）  \n",
    "3、量化过程：$Q(x)=clamp(round(\\frac{x}{s}+z),q_{min} ,q_{max})$  \n",
    "4、反向量化的过程：$x=s\\cdot(Q(x)-z)$  \n",
    "其中，clamp函数是裁剪函数：  \n",
    "$$\n",
    "\\text{clamp}(x, min, max) = \n",
    "\\begin{cases} \n",
    "a & \\text{if } x < min \\\\\n",
    "x & \\text{if } min \\leq x \\leq max \\\\\n",
    "b & \\text{if } x > max \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac681730-d2b3-466b-81d9-97dd562293df",
   "metadata": {},
   "source": [
    "**PTQ（训练后量化 Post-Training Quantization）：**  \n",
    "在模型训练完成后，对已经训练好的模型进行量化的方法。  \n",
    "特点：  \n",
    "1、无需重新训练：直接对预训练模型进行量化。  \n",
    "2、快速部署：量化过程相对简单快速。  \n",
    "3、精度损失：可能存在一定的精度损失。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02b60c-ec52-4738-a4dc-6e2aa910db21",
   "metadata": {},
   "source": [
    "流程：  \n",
    "1、收集统计信息：通过少量校准数据收集模型各层的激活值分布。  \n",
    "2、计算量化参数：根据统计信息计算每层的缩放因子和零点偏移。  \n",
    "3、量化模型参数：将模型权重和激活值量化为低精度数值。  \n",
    "4、验证精度：在验证集上测试量化后模型的精度。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da016d33-acfd-4fda-af17-42674c4eed3b",
   "metadata": {},
   "source": [
    "**QAT（量化感知训练 Quantization-Aware Training）：**  \n",
    "在模型训练过程中模拟量化效果，使模型能够适应量化带来的精度损失。  \n",
    "特点：  \n",
    "1、训练时模拟量化：在训练过程中插入伪量化节点。  \n",
    "2、精度保持：能够更好地保持模型精度。  \n",
    "3、训练成本：需要额外的训练时间和计算资源。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5815e4-9c80-458f-9df2-5d74cae103a9",
   "metadata": {},
   "source": [
    "流程：  \n",
    "1、插入伪量化节点：在模型的权重和激活值上插入伪量化操作。  \n",
    "2、微调训练：使用少量训练数据对模型进行微调。  \n",
    "3、真实量化：训练完成后进行真实的量化操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b9459-5274-4d9c-8453-db99f4b48682",
   "metadata": {},
   "source": [
    "**PTQ和QAT的区别：**  \n",
    "PTQ 是在模型训练完成后，基于**校准数据**对权重和激活值进行一次性量化；而 QAT 是在训练过程中，通过在前向传播的各个模块（如 MHA、FFN）中插入伪量化节点来模拟量化误差，使模型在训练时就能适应量化，从而获得更好的量化后精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7973a741-0b86-466d-a0c9-1078dc940ef2",
   "metadata": {},
   "source": [
    "**量化校准：**  \n",
    "在训练后量化过程中，使用少量有代表性的数据来收集模型各层的统计信息，从而计算出合适的量化参数（缩放因子和零点偏移）的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e797a-9263-4df3-9840-b2ed1608b82e",
   "metadata": {},
   "source": [
    "量化校准方法：  \n",
    "1、最大最小值校准（MinMax Calibration）：  \n",
    "收集校准数据通过模型时各层激活值的最大值和最小值使用这些极值来确定量化范围。  \n",
    "2、百分位数校准（Percentile Calibration）：  \n",
    "收集校准数据的统计分布使用百分位数（如99.99%）来确定量化范围，避免异常值的影响。  \n",
    "3、KL散度校准（KL Divergence Calibration）：  \n",
    "计算不同量化范围下的KL散度选择KL散度最小的量化范围。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bd89e3-9787-42ae-ac1a-fe0f4e98324d",
   "metadata": {},
   "source": [
    "**GPTQ量化方法（Gaussian Process Transformer Quantization）[PTQ]：**  \n",
    "针对Transformer模型的高效量化方法。  \n",
    "特点：  \n",
    "1、逐层量化：逐层对模型进⾏量化，保持精度。  \n",
    "2、海森矩阵：利用海森矩阵信息来优化量化过程。  \n",
    "3、一次性量化：只需要一次前向传播即可完成量化。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72736c-2130-43b9-b105-e64468b305ca",
   "metadata": {},
   "source": [
    "优势：  \n",
    "1、误差最小化：通过最小化量化前后输出的均方误差来优化量化参数。  \n",
    "2、海森信息：利用海森矩阵来衡量不同权重对输出误差的贡献。  \n",
    "3、逐组量化：将权重分组进行量化，提高效率。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70884a9-8b49-418e-becb-b187b9455655",
   "metadata": {},
   "source": [
    "工作原理：  \n",
    "有一个权重矩阵W，量化过程如下  \n",
    "1、计算输出误差：  \n",
    "$$\n",
    "\\Delta=(W-W_{quant})\\cdot H\\cdot(W-W_{quant})^T\n",
    "$$  \n",
    "H是海森矩阵，它的作用是计算每一个权重对模型性能的影响程度。  \n",
    "2、最小化误差（选择最接近原模型能让误差最小的量化值）：  \n",
    "$$\n",
    "W_{quant}=arg\\min_{W_{quant}} \\Delta\n",
    "$$  \n",
    "argmin是使函数取得最小值的自变量（集合）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9782735-0b62-49b8-ac5a-293091a9296f",
   "metadata": {},
   "source": [
    "**AWQ量化方法（Activation-aware Weight Quantization）[PTQ]：**  \n",
    "特点：  \n",
    "1、激活值感知：考虑激活值的分布特点进行量化，适用于激活值分布不均的情况。 \n",
    "2、通道重要性：识别不同通道的重要性，对重要通道使用更高精度。  \n",
    "3、保护重要权重：对重要的权重进行保护，减少量化损失。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59280fce-79f9-4ca8-9f93-3d2fea780d7b",
   "metadata": {},
   "source": [
    "优势：  \n",
    "1、重要性评估：评估不同权重通道的重要性。  \n",
    "2、自适应量化：根据重要性自适应调整量化策略。  \n",
    "3、保护机制：对重要权重使用更精细的量化策略。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6fdc45-090e-43cd-a124-d7292e089a74",
   "metadata": {},
   "source": [
    "工作原理：  \n",
    "1、计算通道重要性：  \n",
    "$$\n",
    "I_c=\\sum_{i,j}|W_{c,i,j}\\cdot A_{i,j}|\n",
    "$$  \n",
    "c指的是卷积核的第c个输出通道。    \n",
    "$W_{c,i,j}$指的是第c个卷积核所在位置$(i,j)$的权重，用于transformer模型的时候，将(i,j)平铺为一个输入维度k。     \n",
    "$A_{i,j}$是输入特征图在位置$(i,j)$的激活值（可能是来自前一层的特征，不一定是原始图像）。    \n",
    "这个公式中的通道指的是模型中间层的特征通道。  \n",
    "\n",
    "2、根据重要性调整量化策略：  \n",
    "重要通道：使用更高精度或特殊保护。    \n",
    "不重要通道：使用标准量化。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
