{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc524582-a407-42b0-a93b-c1dc283fb652",
   "metadata": {},
   "source": [
    "# 位置编码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6cef28-c60e-42d7-958a-ee6e964f20e8",
   "metadata": {},
   "source": [
    "## 一、一些关于位置编码的基本概念："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0a21a7-0a60-40ad-97fd-dd69ecf2751b",
   "metadata": {},
   "source": [
    "1、为什么需要：transformer本身没有位置序列的概念。  \n",
    "2、位置编码的核心作用：给模型提供序列中每一个位置的信息，让模型知道词语的先后顺序。  \n",
    "3、什么是上下文长度：模型能够一次处理的最大序列的长度  \n",
    "4、长度限制的由来：计算复杂度（注意力机制的计算复杂度是O(n^2),序列长度翻倍，计算量翻四倍），内存限制（长序列需要更多内存存储注意力矩阵），位置编码限制（某些位置编码方式再训练时只能看到固定长度，如cos/sin位置编码）  \n",
    "5、什么是外推能力：是指模型再训练时见过短序列，但在推理时能处理较长序列的能力。    \n",
    "6、绝对位置编码：直接编码每一个位置的绝对坐标。  \n",
    "7、相对位置编码：关注位置之间的相对距离而不是绝对位置。  \n",
    "8、相对位置编码的优势：更好的泛化能力，自然语言的特性，长度外推的友好性，现代大模型的广泛选择。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9586912-a34e-42b4-a3c1-94685091a61c",
   "metadata": {},
   "source": [
    "## 二、RoPE位置编码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48337e80-40ec-42f6-ae66-63412e3c0530",
   "metadata": {},
   "source": [
    "见ROPE笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7138983-0882-444c-b7b3-c79e97a433ed",
   "metadata": {},
   "source": [
    "## 三、其他位置编码技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cde2290-59f2-4562-a30e-ce165238e1cb",
   "metadata": {},
   "source": [
    "#### 1、ALIBI(Attention with Linear Biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e6c1f-badc-414c-a387-9968ffcbac68",
   "metadata": {},
   "source": [
    "核心思想：不使用显式的位置编码向量，而是通过在注意力分数（attention scores）上直接添加一个与 token 间相对距离成比例的线性偏置（bias），来隐式地引入相对位置信息，从而提升模型对长序列的外推能力。  \n",
    "工作原理：  \n",
    "对于位置i,j的注意力分数，添加偏置：\n",
    "$b_{ij} = Q_i · K_j + m * |i - j|$  \n",
    "m 是一个负的、固定的斜率（slope），它决定了距离对注意力分数的“惩罚”程度。距离越远，偏置项越负，分数被压得越低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63369eb9-14cd-43e5-a261-1adc0dab83ae",
   "metadata": {},
   "source": [
    "**优势是**  \n",
    "简单高效：不需要额外的位置编码参数  \n",
    "天然适合外推：线性偏执适合长度外推  \n",
    "计算友好：直接在注意力分数上操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10662656-fb92-4dc2-af2a-4747910072ea",
   "metadata": {},
   "source": [
    "#### 2、YARN技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44979447-befc-4a7d-98e7-e5c414b01c83",
   "metadata": {},
   "source": [
    "这是对Rope的改进，专门用于解决长度外推问题。  \n",
    "核心思想：通过调整Rope的频率（基础频率），让模型更好的处理长序列。  \n",
    "技术细节：   \n",
    "1、频率外推与维度重加权：  \n",
    "将基础频率从 θ_i 调整为 θ'_i = θ_i^(1/α)，系统性降低**所有频率**，延缓相位缠绕。  \n",
    "引入可学习的幅度缩放（magnitude），对旋转完成后的向量本身进行缩放，在微调中自动**增强低频维度的贡献**，**抑制高频噪声**，恢复多尺度建模能力。  \n",
    "2、温度缩放：调整注意力分布的温度（**不同于**模型最后生成 token 时，对 logits 进行缩放的那个 T，这里指的是Attention = softmax(Q @ K^T / √d_k) @ V\n",
    "这里的 / √d_k 就是一个**温度缩放因子**）  \n",
    "3、渐进式训练：逐步增加训练长度  \n",
    "可以提升Rope的外推能力8到16倍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738442eb-1608-4b1d-b6e4-f2ace125b36e",
   "metadata": {},
   "source": [
    "YARN 相当于对所有频率的位置编码都“减慢了旋转速度”，或者说“压缩了旋转的角度”。它并没有加大低频位置的转速，反而是让高频和低频的旋转都变得更慢了，但这种“减速”对高频成分的影响更大，从而相对地“保留”了低频信息。  \n",
    "原始角度：在位置 m，旋转角度是 $m * θ_i $   \n",
    "YARN 调整后：相当于使用 m/γ 来计算角度，即 $(m/γ) * θ_i  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af437ebb-7f50-4074-9652-a11712f426e5",
   "metadata": {},
   "source": [
    "#### 3、NTK-aware设计原则"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a3aefa-2d1c-4a78-b8d2-46c52ffa9a27",
   "metadata": {},
   "source": [
    "这是一种改进技术，用于改进Rope，使其能够更好的适应外推。  \n",
    "NTK理论表明：神经网络在学习高频和低频函数时有着不同的表现。  \n",
    "低频函数（如长距离依赖）：学习速度快，但需要更大的网络容量  \n",
    "⾼频函数（如短距离依赖）：学习速度慢，但网络容量需求小  \n",
    "NTK-aware：先根据NTK理论预演长序列学习状态，再调整现有Rope参数来逼近这个值。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ff804-f693-4e59-bb9e-c38036194b46",
   "metadata": {},
   "source": [
    "**NTK-aware解决方案：**  \n",
    "根据NTK理论重新分配\"学习容量\"，让低频维度获得更多资源  \n",
    "1、**频率缩放：**  \n",
    "核心思想：通过引入一个缩放因子 α（通常 α > 1），来调整 RoPE 中的基础频率 $θ_i$，使其适应更长的序列。其理论依据是：当序列变长时，理想的频率分布应该发生变化，以匹配该长度下模型的 NTK 特性。    \n",
    "标准 NTK-aware：$θ'_i = α * θ_i$\t均匀缩放\t简单直接，所有维度同等增强。  \n",
    "YARN-style：$θ'_i = θ_i^{\\frac{1}{α}}$\t非线性缩放\t等效于位置压缩，减缓所有旋转。  \n",
    "NTK-scaling：$θ'_i = θ_i × (1 + α × (\\frac{i}{d})^β)$\t非均匀缩放\t可配置地增强特定频段（如低频），更灵活。   \n",
    " α = 0.3（低频增强因子）β = 0.75（非线性缩放指数）d是d_model  \n",
    "2、**维度重要性重加权：**  \n",
    "核心思想：在注意力机制中，并非所有维度都同等重要。我们可以通过为不同维度分配不同的权重，来引导模型更关注那些对任务（尤其是长距离依赖）更有用的维度信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075b059e-4752-43c3-8697-aa0b4238c65a",
   "metadata": {},
   "source": [
    "NTK-aware 回答了“为什么”要调整频率（理论依据），而 YARN 回答了“怎么做”来调整频率并成功外推（工程实现），YARN 的做法是符合 NTK-aware 思想的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeaae61-6ccb-411b-bc11-3a1fc4ade31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
