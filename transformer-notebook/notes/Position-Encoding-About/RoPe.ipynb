{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330ac08c-792c-4141-83e8-edf6f218f741",
   "metadata": {},
   "source": [
    "# 旋转位置编码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7704f84-5678-48fb-9c5e-11cca7d454da",
   "metadata": {},
   "source": [
    "### 一、传统位置编码的缺陷："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abab33e-7849-48a0-a4e9-7176e77fb5b4",
   "metadata": {},
   "source": [
    "传统的Transformer模型使用的是固定的、基于正弦和余弦函数的绝对位置编码。这种编码方式之所以只能表示绝对位置且难以泛化到更长序列，有如下原因：  \n",
    "1、绝对位置的本质：  \n",
    "$$PE_(pos,2i)=sin(\\frac{pos}{10000^\\frac{2i}{d_\\text{model}}})$$\n",
    "$$PE_(pos,2i+1)=cos(\\frac{pos}{10000^\\frac{2i}{d_\\text{model}}})$$  \n",
    "此处的pos是绝对位置索引，这通常意味着：  \n",
    "模型学到的是“第3个位置的词”有什么含义，而不是“在主语之后两个位置的词”这样的**相对关系**。  \n",
    "每个位置的编码是固定的，与**上下文无关**。  \n",
    "模型无法自然地理解“距离”或“顺序”的相对概念，除非通过大量训练隐式学习。    \n",
    "2、难以泛化到更长序列：  \n",
    "传统位置编码是预定义的、固定长度的查找表或可计算但范围受限的函数。这导致：  \n",
    "**训练时的最大长度限制**：如果模型在最大长度为512的序列上训练，那么它只见过 pos=0到 511的位置编码。  \n",
    "**编码值可能“爆炸”或“失真”**：虽然正弦函数本身是周期性的，但不同频率的组合在超出训练范围后，其向量表示可能变得稀疏或与其他位置高度相似，导致模型无法正确区分位置。  \n",
    "**推理时超出范围**：当输入序列长度超过训练时的最大长度，模型会遇到从未见过的位置索引  \n",
    "**缺乏外推能力**：模型没有机制来“推断”一个更长位置的含义，因为它没有学习到如何根据相对距离或模式生成新位置的表示。  \n",
    "3、缺乏相对位置感知：  \n",
    "自然语言中的许多结构依赖于相对位置，传统PE不显式编码这些相对信息。虽然Transformer的自注意力机制理论上可以通过注意力权重学习相对位置，但原始的正弦PE并不直接提供这种归纳偏置，导致模型需要从数据中从零学习，效率较低。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0953882c-e796-4602-bf80-f7b1441a942b",
   "metadata": {},
   "source": [
    "### 二、Rope旋转编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6019dfd9-a9e9-4618-9592-50e5554c235e",
   "metadata": {},
   "source": [
    "核心思想：相对位置编码技术，通过旋转来编码相对位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd59f0-39d2-4da1-8fff-3e87d4e3b20c",
   "metadata": {},
   "source": [
    "rope的复数本质：  \n",
    "数学本质是将每一个**维度对**视为一个复数，位置编码=乘以单位复数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81c1ae-c801-4785-ace4-29c3ea3d471b",
   "metadata": {},
   "source": [
    "rope的优势：  \n",
    "1、保留绝对位置编码的简洁性，  \n",
    "2、实现相对位置编码的效果，  \n",
    "3、并且天然支持超长上下文外推。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d5802-1ef9-4ff4-a3be-1f33ff45158a",
   "metadata": {},
   "source": [
    "**rope的核心编码思想**：将位置信息编码为向量的旋转角度 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f0493-e7ad-4d0e-8e84-9120fe7d7a1d",
   "metadata": {},
   "source": [
    "**RoPE 是在每个 token 独立处理时完成的。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3eb247-bc25-40dd-8bbc-be21ebd95398",
   "metadata": {},
   "source": [
    "**二维向量旋转的基本概念：**  \n",
    "在二维平面上，一个向量 [x, y] 绕原点逆时针旋转角度 θ 后，新坐标为（用极坐标推理出来的）：  \n",
    "x' = x·cosθ - y·sinθ  \n",
    "y' = x·sinθ + y·cosθ    \n",
    "写成矩阵形式：  \n",
    "[x'] = [cosθ-sinθ] [x]  \n",
    "[y'] = [sinθ+cosθ] [y]  \n",
    "其中    \n",
    "[cosθ-sinθ]    \n",
    "[sinθ+cosθ]  就是旋转矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a51f93-9b20-4ed6-bff9-787ca0cdf4df",
   "metadata": {},
   "source": [
    "**rope旋转角度计算公式：** token位置乘上频率 \n",
    "$$\n",
    "\\theta_i = \\frac{m}{base^\\frac{2i}{d}}\n",
    "$$\n",
    "其中，base一般设定为10000，m为token的位置，i为维度对的索引（第i个偶数维度），d是模型维度d_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440930a8-14ee-4c19-91fb-8288196482a0",
   "metadata": {},
   "source": [
    "**为什么rope具有良好的外推性：**  \n",
    "1、旋转的周期性：位置编码天然有$2\\pi $的周期，不会溢出或退化   \n",
    "2、rope满足相对位置性质$f(x,m) \\cdot f(y,n) = f(x \\cdot y,m-n)$，这意味着**计算注意力分数**（这个公式）只依赖于相对位置m-n，不依赖于绝对位置m,n。  \n",
    "其中该公式:   \n",
    "x：某个 token 的 query 向量（来自W_q变换）  \n",
    "y：另一个 token 的 key 向量（来自W_k变换）  \n",
    "计算的是他俩之间的注意力分数  \n",
    "3、频率的层次结构：  \n",
    "低维高频处理短距离关系  \n",
    "高维低频处理长距离关系  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ca4dc-d63b-4d40-a626-4d7e6370ebd3",
   "metadata": {},
   "source": [
    "**为什么Rope在超长上下文外推能力会失效：**  \n",
    "相位缠绕：长序列导致旋转角度过大，会导致落在$2\\pi$整数倍附近的风险加大，容易导致角度巧合重合，位置信息混淆。旋转是周期性的，每过$2\\pi$就会回到原点。  \n",
    "方差爆炸：注意力分数方差会随着序列长度的增加而增加，注意力分数分布失衡，导致“注意力崩溃”。  \n",
    "频谱失衡：高频成噪声，低频失效，多尺度建模能力丧失。  \n",
    "分布偏移：推理状态远超训练经验，模型行为失控。  \n",
    "有限外推：外推能力随长度增加而指数级衰减。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6fa45-d1ec-40e8-b127-c73ebf37e473",
   "metadata": {},
   "source": [
    "#### 如何旋转："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb3225d-ccf3-41af-84c0-ba6e702c7195",
   "metadata": {},
   "source": [
    "1、生成Q和K：    \n",
    "```Q = Q.view(batch, len, n_heads, d_head).transpose(1, 2)  # [batch, n_heads, len, d_head]  ```  \n",
    "```K = K.view(batch, len, n_heads, d_head).transpose(1, 2)  # [batch, n_heads, len, d_head]  ```  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e314281-8560-424a-8cdd-db4c44aff040",
   "metadata": {},
   "source": [
    "将其**看作**多头的形式（使用view函数没有复制开销，极其高效而且不易出错，直接切分则低效而容易出错）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7402a4-9a70-414c-85a9-d8f639390749",
   "metadata": {},
   "source": [
    "2、预计算“旋转频率表” freqs[i]（这是ROPE的设定，只算一次，全局共享，也称频率基）：   \n",
    "freqs = 1.0 / (10000 ** (torch.arange(0, d_head, 2) / d_head))   \n",
    "我们将 d_head 维度按相邻两两分组，视为 $\\frac{\\text{d\\_head}}{2}$ 个二维向量$(x_i,y_i)$，每个二维向量将在旋转操作中作为一个整体进行旋转变换。  \n",
    "freqs[i] 是根据维度的位置预先设定的基频率，同一个模型中所有 token、所有注意力头、所有层，只要处于相同的维度位置（如第 0-1 维、第 2-3 维等），就使用相同的 freqs[i]。  \n",
    "对于一个d_head，里面所有token的freqs[i]都是一样的  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c0bd92-57b4-40c5-8d98-7ceb400c745c",
   "metadata": {},
   "source": [
    "freqs[i] 越大 → 高频 → 对位置敏感（精细定位）  \n",
    "freqs[i] 越小 → 低频 → 对位置迟钝（粗粒度定位）  \n",
    "低维度（i=0）：旋转快（高频）→ 编码短距离关系    \n",
    "高维度（i=d/2-1）：旋转慢（低频）→ 编码长距离关系  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a65e7c-b0dd-4287-9f6b-6ebaaff39f5b",
   "metadata": {},
   "source": [
    "3、为每一个位置计算旋转角度：  \n",
    "引入位置m（每一个头的每一个token的绝对位置[0,1,...len-1]）  \n",
    "对每一个位置m计算它的旋转角度：$\\theta_m = m * freqs$ ，每个位置 m 得到不同的 $\\theta_m$  \n",
    "```positions = torch.arange(len)                    # [len]```获取当前的位置索引   \n",
    "```angles = torch.outer(positions, freqs)           # [len, d_head//2]```求出了$\\theta_m$  \n",
    "```cos = torch.cos(angles)                          # [len, d_head//2]```  \n",
    "```sin = torch.sin(angles)                          # [len, d_head//2]```这是在计算旋转公式需要的值   \n",
    "现在 cos[m, i] 和 sin[m, i] 告诉我们：在位置 m，第 i 个二维对应该旋转多少度。   \n",
    "接下来扩展维度为后续广播做准备：   \n",
    "```cos = cos.unsqueeze(0).unsqueeze(0)  # [1, 1, len, d_head//2]```   \n",
    "```sin = sin.unsqueeze(0).unsqueeze(0)  # [1, 1, len, d_head//2]```   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468696f0-0449-439a-9001-ea7e87d9a531",
   "metadata": {},
   "source": [
    "4、将 d_head 维向量看作 d_head//2 个二维对：    \n",
    "```# Q: [batch, n_heads, len, d_head]```  \n",
    "```Q = Q.view(batch, n_heads, len, d_head // 2, 2)  # [batch, n_heads, len, d_head//2, 2]```  \n",
    "```K = K.view(batch, n_heads, len, d_head // 2, 2)```    \n",
    "最后一维 [2] 就是 (x, y)  \n",
    "倒数第二维 d_head//2 是“有多少个对”  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041bb83-77d5-40c9-813a-977da0e39d38",
   "metadata": {},
   "source": [
    "5、应用旋转公式：  \n",
    "前期Q和K的维度是[batch, n_heads, len, d_head//2, 2] 2代表一个二维对（x,y)现在需要进行分离。   \n",
    "```Q_x, Q_y = Q[..., 0], Q[..., 1]```  \n",
    "```K_x, K_y = K[..., 0], K[..., 1]```     \n",
    "接下来应用旋转公式：  \n",
    "```Q_rot_x = Q_x * cos - Q_y * sin```  \n",
    "```Q_rot_y = Q_x * sin + Q_y * cos```  \n",
    "  \n",
    "```K_rot_x = K_x * cos - K_y * sin```  \n",
    "```K_rot_y = K_x * sin + K_y * cos```  \n",
    "接下来重新组合为二维向量的结构：  \n",
    "```Q_rot = torch.stack([Q_rot_x, Q_rot_y], dim=-1)  # [batch, n_heads, len, d_head//2, 2]```    \n",
    "```K_rot = torch.stack([K_rot_x, K_rot_y], dim=-1)```    \n",
    "因为一直都是**看作**，现在已经旋转完毕，看回来，变为原始形状：  \n",
    "```Q_rot = Q_rot.view(batch, n_heads, len, d_head)  # [batch, n_heads, len, d_head]```  \n",
    "```K_rot = K_rot.view(batch, n_heads, len, d_head)```  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aaa39e-0043-455c-ae46-a1cc3508f209",
   "metadata": {},
   "source": [
    "6、计算注意力分数：  \n",
    "现在 Q_rot 和 K_rot 已经注入了位置信息，可以直接用于注意力：  \n",
    "```attn_scores = torch.matmul(Q_rot, K_rot.transpose(-2, -1)) / sqrt(d_head)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c300098-651c-4e02-b65c-bb978974a751",
   "metadata": {},
   "source": [
    "### 三、RopE旋转编码的缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20008aef-7e81-468f-8bcd-e45f2458198d",
   "metadata": {},
   "source": [
    "1、低维高频旋转混乱问题：  \n",
    "实际问题：当处理长序列时，低维度（高频）会旋转过多圈数，导致位置信息混乱。  \n",
    "2、高维低频分辨率不足：  \n",
    "高维度（低频）在长距离上的分辨率不够，无法精确区分相邻位置。有些时候相近位置差异过小，会导致无法区分，在理解精确任务的时候容易出错。  \n",
    "3、训练长度和外推能力的矛盾：  \n",
    "rope在训练长度内表现良好，但在外推到训练序列的一定倍数的长度后，性能会急剧下降。  \n",
    "4、维度利用不均衡：    \n",
    "不同维度的作用差异巨大，导致模型容量浪费。    \n",
    "高频维度（低维）过多但用处有限  \n",
    "低频维度（高维）过少但需求量大      \n",
    "中频维度的覆盖范围过窄  \n",
    "5、对注意力模式的限制：  \n",
    "rope的固定旋转模式使得每一层都使用相同的位置编码，这样就让多层注意力机制失效了，每一层都无法获得最合适的位置信息。  \n",
    "6、计算复杂度的问题：增加了计算开销  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
