{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86db0be-9265-4bd2-9d8e-d9f6f6255aaf",
   "metadata": {},
   "source": [
    "## Transformer经典问题："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0459413a-1036-48f7-b640-6c984ce4a70d",
   "metadata": {},
   "source": [
    "1、transformer整体架构是怎么样的，几个部分组成？    \n",
    "**输入处理**：词嵌入 + 位置编码（正弦函数），并添加 `<sos>`、`<eos>`、`<pad>` 特殊标记。  \n",
    "**编码器（6层）**：每层含多头自注意力（带填充掩码）和前馈网络（FFN），每子层后接残差连接与层归一化。  \n",
    "**解码器（6层）**：每层含掩码自注意力（填充+前瞻掩码）、交叉注意力（Q来自解码器，K/V来自编码器）、FFN，每子层后同样接残差+层归一化。  \n",
    "**输出处理**：线性投影（维度扩展至词表大小，权重与输入词嵌入共享） + Softmax，输出各词概率分布。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0631375-9bd8-4841-b115-b6bc1546c5f9",
   "metadata": {},
   "source": [
    "2、介绍transformer，详细介绍QKV过程？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ecb1-5518-4a71-a585-ab61abc8e869",
   "metadata": {},
   "source": [
    "序列X分别与权重矩阵相乘，得到Q,K,V。在多头注意力计算中，每一个头独立计算。  \n",
    "拼接各头输出后乘W_0得到结果，注意力机制通过softmax(Q·K^T / sqrt(d_k))·V来实现词间动态关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b243ba5-71fd-4040-b231-9062a3e0e484",
   "metadata": {},
   "source": [
    "3、讲⼀讲tansformer里的encoder和decoder，以及整体工作原理，还有交叉注意力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a7610-af48-4496-bc1d-7381d2dbdaf5",
   "metadata": {},
   "source": [
    "编码器：通过多层自注意力和 FFN，将输入序列编码为富含上下文的表示。  \n",
    "解码器：自回归生成目标序列，每步通过自注意力关注已生成内容，通过交叉注意力查询编码器输出。  \n",
    "交叉注意力：Q 来自解码器，K/V 来自编码器，实现“解码时动态检索源信息”，完成语义对齐。  \n",
    "整体流程：编码器输出作为“知识库”，解码器边生成边查询，实现高质量序列生成。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88e2a0-dacc-49a0-b88a-2dd54387f2f9",
   "metadata": {},
   "source": [
    "4、transformer计算过程，softmax为什么要进⾏缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afcabb0-3d4c-4e9c-9bfd-4e7f614c6762",
   "metadata": {},
   "source": [
    "缩放是为了防止梯度消失。\n",
    "QK^T 的方差随 d_k增大而增大，导致 softmax 输入值过大，输出接近 one-hot，梯度趋近于零。\n",
    "除以 sqrt(d_k)可稳定方差，使 softmax 输入处于敏感区，保证梯度有效，训练更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbde345-768a-4273-b1b0-e7eacf75bda5",
   "metadata": {},
   "source": [
    "5、解释前馈神经网络（FFN）在Transformer中的作用及其设计理念"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee912d3-4897-41ce-86c3-bbff994c7ced",
   "metadata": {},
   "source": [
    "FFN使用两个全连接层加一个RELU激活函数  \n",
    "作用：在注意力机制后引入非线性变换，增强模型表达能力。  \n",
    "设计理念：先升维扩大特征空间以捕获复杂模式，再降维回归原始维度，保留关键信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82fc00-ed6d-4961-b6ea-362e83244e8e",
   "metadata": {},
   "source": [
    "6、为什么FFN要将高维度映射回低维度去呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9a85dc-fe0b-44af-9a1d-00693baf4080",
   "metadata": {},
   "source": [
    "升维是为了在高维空间中更好地分离和学习复杂特征；\n",
    "降维是为了压缩信息、减少冗余，并将特征映射回模型主干的 d_model,d_ model维度，保持结构一致性，避免参数爆炸。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7672d20-ffe7-4649-8cca-55f060be725b",
   "metadata": {},
   "source": [
    "7、什么是残差连接，解释残差连接（Residual Connection）在Transformer中的作用及必要性"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a794c8-2bf6-41d5-b66a-332f473c5e45",
   "metadata": {},
   "source": [
    "残差连接就是用上一层的输入加上本层的输出, y=x+sublayer(x)   \n",
    "作用：缓解深层网络中的梯度消失，使信息和梯度可直接跨层传播。  \n",
    "必要性：Transformer 堆叠 6 层以上，残差连接确保深层模型仍可有效训练，加速收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ab157-96e8-478a-acbf-0d0d537e6a22",
   "metadata": {},
   "source": [
    "8、FFN块的计算公式是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c76926-f3f3-4b02-ba52-d2950c68ee5a",
   "metadata": {},
   "source": [
    "$$\n",
    "FFN(x) = max(0,x\\cdot W_1+b_1)\\cdot W_2+b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd1806-107c-4416-b4be-79a3505bf468",
   "metadata": {},
   "source": [
    "9、encoder和decoder中掩码的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18705ac7-cd67-4fbe-bec4-cc26504ee3d8",
   "metadata": {},
   "source": [
    "编码器：仅使用 填充掩码，屏蔽 padding 位置，防止无效信息干扰。  \n",
    "解码器：使用 双重掩码：  \n",
    "自注意力：前瞻掩码 + 填充掩码，防止看到未来词和 padding；  \n",
    "交叉注意力：填充掩码，确保只关注编码器的有效输入  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee3100b-10a1-482d-a32f-c5bd3b814fc2",
   "metadata": {},
   "source": [
    "10、Transformer怎么处理文字输入的？以及Transformer的输出是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfeddaf-f897-446f-a516-ba2f4e431dff",
   "metadata": {},
   "source": [
    "输入处理：文本 tokenization 后查表转为词嵌入，加上正弦位置编码，形成d_model,d_model维输入向量。  \n",
    "输出：解码器输出经线性层 + Softmax，生成一个概率分布向量，表示词汇表中每个词在当前位置的出现概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d959dc5-de6b-47f3-aa61-a68fc0ade1f4",
   "metadata": {},
   "source": [
    "## 代码作业1：实现基础缩放点积注意力（Scaled Dot-Product Attention）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f27f1-a298-4021-8969-ed3dbedfb415",
   "metadata": {},
   "source": [
    "要求：  \n",
    "基于教案中的注意⼒计算⽰例，实现⼀个基础的缩放点积注意⼒机制。你需要完善下列代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c43bb5d-3026-45b2-909f-bc203d302e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "def scaled_dot_product_attention_demo(query, key, value, mask=None, dropout=0.0):\n",
    "    \"\"\"\n",
    "    计算缩放点积注意力\n",
    "    \n",
    "    参数:\n",
    "        query: 查询张量，形状为 (batch_size, seq_len, d_k)\n",
    "        key: 键张量，形状为 (batch_size, seq_len, d_k)\n",
    "        value: 值张量，形状为 (batch_size, seq_len, d_v)\n",
    "        mask: 可选的掩码张量，形状为 (batch_size, seq_len, seq_len)\n",
    "        dropout: dropout概率\n",
    "    \n",
    "    返回:\n",
    "        output: 注意力输出，形状为 (batch_size, seq_len, d_v)\n",
    "        attention_weights: 注意力权重，形状为 (batch_size, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # 请在这里实现注意力计算\n",
    "    # 步骤：\n",
    "    # 1. 计算Q和K的点积，除以sqrt(d_k)进行缩放\n",
    "    # 2. 如果提供了掩码，将掩码位置的值设为负无穷大\n",
    "    # 3. 应用softmax函数得到注意力权重\n",
    "    # 4. 如果提供了dropout，应用到注意力权重上\n",
    "    # 5. 用注意力权重对V进行加权求和\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c29ba-aa1d-4c6e-8914-2e23a820ccb4",
   "metadata": {},
   "source": [
    "**这是我自己写的版本：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a5a53f-fe32-46c6-92ee-32a4ee3f4c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention_my(query, key, value, mask=None, dropout=0.0):\n",
    "    # 1. 计算Q和K的点积，除以sqrt(d_k)进行缩放\n",
    "    scores = torch.matmul(query, key.transpose(-1, -2))/math.sqrt(query.size(-1))\n",
    "    # 2. 如果提供了掩码，将掩码位置的值设为负无穷大\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(~mask, -float('inf'))\n",
    "    # 3. 应用softmax函数得到注意力权重\n",
    "    attention_weights = F.softmax(scores,dim=-1)\n",
    "    # 4. 如果提供了dropout，应用到注意力权重上\n",
    "    if dropout != 0.0:\n",
    "       attention_weights = nn.Dropout(attention_weights)\n",
    "    # 5. 用注意力权重对V进行加权求和\n",
    "    output = torch.matmul(attention_weights,value)\n",
    "    \n",
    "    return output,attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e80ab-cb40-4ce4-baec-d86917a9ff83",
   "metadata": {},
   "source": [
    "**接下来是使用AI修正后的版本：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82636cf9-5509-4cad-b084-9229625eb3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=0.0):\n",
    "    # 1. 计算 Q @ K^T / sqrt(d_k)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "\n",
    "    # 2. 应用掩码（如果提供）\n",
    "    if mask is not None:\n",
    "        if mask.dtype != torch.bool:\n",
    "            mask = mask.bool()\n",
    "        scores = scores.masked_fill(~mask, -float('inf'))\n",
    "\n",
    "    # 3. Softmax 得到注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 4. 应用 dropout\n",
    "    if dropout > 0.0:\n",
    "        attention_weights = F.dropout(attention_weights, p=dropout, training=True)\n",
    "\n",
    "    # 5. 加权求和得到输出\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408d3da-6330-4a7e-84b8-18f092c260d7",
   "metadata": {},
   "source": [
    "**以下是测试代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a196956-9b69-4e9f-9822-22a00d7c469a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试基础注意力 ===\n",
      "Q形状: torch.Size([2, 4, 64])\n",
      "K形状: torch.Size([2, 4, 64])\n",
      "V形状: torch.Size([2, 4, 64])\n",
      "输出形状: torch.Size([2, 4, 64])\n",
      "注意力权重形状: torch.Size([2, 4, 4])\n",
      "输出均值: -0.1191\n",
      "注意力权重行和: 1.0000\n",
      "\n",
      "=== 测试带掩码的注意力 ===\n",
      "掩码形状: torch.Size([2, 4, 4])\n",
      "带掩码输出形状: torch.Size([2, 4, 64])\n",
      "带掩码注意力权重形状: torch.Size([2, 4, 4])\n",
      "被掩码位置的注意力权重: [0.5975434184074402, 0.4024565815925598]\n",
      "✓基础测试通过！\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "def test_scaled_dot_product_attention():\n",
    "    # 设置参数\n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    d_k = 64  # 键的维度\n",
    "    d_v = 64  # 值的维度\n",
    "\n",
    "    # 创建测试数据\n",
    "    Q = torch.randn(batch_size, seq_len, d_k)\n",
    "    K = torch.randn(batch_size, seq_len, d_k)\n",
    "    V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "    print(\"=== 测试基础注意力 ===\")\n",
    "    output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "    print(f\"Q形状: {Q.shape}\")\n",
    "    print(f\"K形状: {K.shape}\")\n",
    "    print(f\"V形状: {V.shape}\")\n",
    "    print(f\"输出形状: {output.shape}\")\n",
    "    print(f\"注意力权重形状: {attention_weights.shape}\")\n",
    "    print(f\"输出均值: {output.mean():.4f}\")\n",
    "    print(f\"注意力权重行和: {attention_weights.sum(dim=-1)[0, 0]:.4f}\")  # 应该接近1.0\n",
    "\n",
    "    # 验证输出形状\n",
    "    assert output.shape == (batch_size, seq_len, d_v), f\"输出形状错误: {output.shape}\"\n",
    "    assert attention_weights.shape == (batch_size, seq_len, seq_len), f\"注意力权重形状错误: {attention_weights.shape}\"\n",
    "\n",
    "    print(\"\\n=== 测试带掩码的注意力 ===\")\n",
    "    # 创建掩码（模拟序列长度分别为2和3）\n",
    "    mask = torch.tensor([\n",
    "        [[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 1, 1]],  # 第1个样本，只关注前2个位置\n",
    "        [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1]]  # 第2个样本，只关注前3个位置\n",
    "    ]).bool()\n",
    "\n",
    "\n",
    "    output_masked, attention_weights_masked = scaled_dot_product_attention(Q, K, V, mask)\n",
    "    print(f\"掩码形状: {mask.shape}\")\n",
    "    print(f\"带掩码输出形状: {output_masked.shape}\")\n",
    "    print(f\"带掩码注意力权重形状: {attention_weights_masked.shape}\")\n",
    "    # 验证掩码效果 - 被掩码的位置注意力权重应该接近0\n",
    "    masked_positions = attention_weights_masked[0, 0, 2:4]  # 第1个样本，第1行的被掩码位置\n",
    "    print(f\"被掩码位置的注意力权重: {masked_positions.tolist()}\")\n",
    "    print(\"✓基础测试通过！\")\n",
    "if __name__ == \"__main__\":\n",
    "    test_scaled_dot_product_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9447e-2ef2-4ef9-a224-329cf38a9c02",
   "metadata": {},
   "source": [
    "## 代码作业2：实现前馈神经网络（FFN）模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9c1a7-9c8f-4eec-b24c-5b38a45dd82b",
   "metadata": {},
   "source": [
    "要求：基于教案中的FFN计算示例，实现⼀个完整的前馈神经网络模块。你需要完善下列代码：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e259bb-88be-4949-9df2-febca07e700e",
   "metadata": {},
   "source": [
    "**以下是我写的：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c0616ae-b7d7-42c7-9ad7-47d17aeeb754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        # 请在这里实现初始化代码\n",
    "        # 要求：\n",
    "        # 1. 第一个线性层：d_model -> d_ff\n",
    "        self.Linear1 = nn.Linear(d_model,d_ff)\n",
    "        # 2. 第二个线性层：d_ff -> d_model\n",
    "        self.Linear2 = nn.Linear(d_ff,d_model)\n",
    "        # 3. 添加dropout层\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请在这里实现前向传播代码\n",
    "        # 步骤：\n",
    "        # 1. 第一个线性变换\n",
    "        x = self.Linear1(x)\n",
    "        # 2. ReLU激活函数\n",
    "        x = F.relu(x)\n",
    "        # 3. Dropout\n",
    "        x = self.Dropout(x)\n",
    "        # 4. 第二个线性变换\n",
    "        x = self.Linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 扩展要求：实现带残差连接和层归一化的FFN模块\n",
    "class TransformerFFN(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(TransformerFFN, self).__init__()\n",
    "        # 请在这里实现初始化代码\n",
    "        # 要求：\n",
    "        # 1. 包含FeedForwardNetwork\n",
    "        self.FFn = FeedForwardNetwork(d_model,d_ff,dropout)\n",
    "        # 2. 添加层归一化（LayerNorm）\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        # 3. 添加dropout用于残差连接\n",
    "        self.Dropout = nn.Dropout(p=dropout)\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 请在这里实现前向传播代码\n",
    "        # 步骤：\n",
    "        residual = x\n",
    "        # 1. 层归一化\n",
    "        x = self.norm(x)\n",
    "        # 2. FFN计算\n",
    "        x = self.FFn(x)\n",
    "        # 3. Dropout\n",
    "        x = self.Dropout(x)\n",
    "        # 4. 残差连接\n",
    "        return residual + x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321f92be-9c4e-4a0f-9648-a6b9cb6be0eb",
   "metadata": {},
   "source": [
    "**以下是测试代码：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58ff6c0-06e5-49a5-aeb6-ec518370346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 测试基础FFN模块 ===\n",
      "输入形状: torch.Size([2, 10, 512])\n",
      "输出形状: torch.Size([2, 10, 512])\n",
      "维度扩展比例: 4.0\n",
      "输出均值: 0.0188\n",
      "输出标准差: 0.2485\n",
      "\n",
      "=== 测试带残差连接的FFN模块 ===\n",
      "输入形状: torch.Size([2, 10, 512])\n",
      "输出形状: torch.Size([2, 10, 512])\n",
      "输出均值: -0.0036\n",
      "输出标准差: 1.0336\n",
      "\n",
      "=== 维度变换分析 ===\n",
      "输入维度: 512\n",
      "中间维度: 2048\n",
      "输出维度: 512\n",
      "扩展比例: 4.0\n",
      "✓测试通过！\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "def test_ffn_modules():\n",
    "    # 设置参数\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    d_model = 512\n",
    "    d_ff = 2048  # 4倍扩展\n",
    "\n",
    "    # 创建模型\n",
    "    ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "    transformer_ffn = TransformerFFN(d_model, d_ff)\n",
    "\n",
    "    # 创建测试数据\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    print(\"=== 测试基础FFN模块 ===\")\n",
    "    output1 = ffn(x)\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {output1.shape}\")\n",
    "    print(f\"维度扩展比例: {d_ff / d_model}\")\n",
    "    print(f\"输出均值: {output1.mean():.4f}\")\n",
    "    print(f\"输出标准差: {output1.std():.4f}\")\n",
    "\n",
    "    print(\"\\n=== 测试带残差连接的FFN模块 ===\")\n",
    "    output2 = transformer_ffn(x)\n",
    "    print(f\"输入形状: {x.shape}\")\n",
    "    print(f\"输出形状: {output2.shape}\")\n",
    "    print(f\"输出均值: {output2.mean():.4f}\")\n",
    "    print(f\"输出标准差: {output2.std():.4f}\")\n",
    "\n",
    "    # 验证输出形状\n",
    "    assert output1.shape == (batch_size, seq_len, d_model), f\"基础FFN输出形状错误: {output1.shape}\"\n",
    "    assert output2.shape == (batch_size, seq_len, d_model), f\"TransformerFFN输出形状错误: {output2.shape}\"\n",
    "\n",
    "    # 验证维度变换\n",
    "    print(f\"\\n=== 维度变换分析 ===\")\n",
    "    print(f\"输入维度: {d_model}\")\n",
    "    print(f\"中间维度: {d_ff}\")\n",
    "    print(f\"输出维度: {d_model}\")\n",
    "    print(f\"扩展比例: {d_ff / d_model}\")\n",
    "\n",
    "    print(\"✓测试通过！\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_ffn_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd826c-2111-4b83-8d7b-1cce6b2973cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
