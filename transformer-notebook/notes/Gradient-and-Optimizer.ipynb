{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4825141e-a575-4ac2-8744-e45d96827003",
   "metadata": {},
   "source": [
    "# 梯度与优化器学习记录笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e910c8a-2fc0-4682-a14a-cdd808985795",
   "metadata": {},
   "source": [
    "**配合教案使用，不再全部记录，主要记录每一个板块产生的疑问和精华**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8b608d-1fef-4de8-bfa3-e95edfbf3feb",
   "metadata": {},
   "source": [
    "## 一、梯度及其产生的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d2580-030d-4c14-9b08-09335c528719",
   "metadata": {},
   "source": [
    "**梯度的定义：**  \n",
    "梯度是多元函数在某一点处变化率最大的方向向量，它指示了函数值增长最快的方向。在深度学习中，\n",
    "梯度通常指损失函数相对于每个模型参数的偏导数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428dd2d-776b-4043-a7cd-e108ac5898fd",
   "metadata": {},
   "source": [
    "**梯度更新的基本原理：**  \n",
    "梯度下降是优化算法的核⼼思想，通过沿着梯度的反方向更新参数来最小化损失函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca7c3d-7b4c-444f-95ad-5d81a7a35134",
   "metadata": {},
   "source": [
    "**梯度下降的三种变体：**  \n",
    "1、批量梯度下降。  \n",
    "2、随机批量梯度下降。（SGD）  \n",
    "3、小批量梯度下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801477f8-c75c-4e45-a80e-712f05a8a451",
   "metadata": {},
   "source": [
    "**梯度在LLM训练中的重要性：**  \n",
    "1、参数更新：梯度指导参数如何减少来调整预测错误。  \n",
    "2、学习信号：模型从数据中获取反馈的机制。当模型预测错误时，计算出的梯度就包含了“哪里错了”以及“应该朝哪个方向修正”的信息。  \n",
    "3、特征学习：通过反向传播，梯度帮助模型学习有用的更复杂的特征表示。这是LLM能够理解语言、生成连贯文本的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc386e24-910a-4929-a6ff-4eed5b6c7958",
   "metadata": {},
   "source": [
    "**梯度消失：**  \n",
    "在深层网络中，梯度在反向传播过程中逐渐变小，最终接近于0，导致网络浅层参数几乎不更新。  \n",
    "原因一般是：  \n",
    "1、激活函数饱和：如sigmoid函数在输入过大过小的时候接近于0.  \n",
    "2、网络层数过深：梯度连乘导致数值越来越小。  \n",
    "3、权重初始化不当：初始权重过小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62852e7-c22e-4a5c-8b92-359776fb8e82",
   "metadata": {},
   "source": [
    "影响一般是：  \n",
    "1、前层网络学习缓慢或停滞。  \n",
    "2、模型无法学习到复杂特征表示。  \n",
    "3、训练效果不佳"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04ccd4a-9362-40b3-9277-1a4bd16e97b0",
   "metadata": {},
   "source": [
    "**梯度爆炸：**  \n",
    "在深层网络中，梯度在方向传播过程中逐渐变大，最终导致数值溢出和参数更新过大。   \n",
    "原因一般是：  \n",
    "1、权重初始化过大  \n",
    "2、学习率设置过高  \n",
    "3、网络结构设计问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f82368-ef5a-4c45-867f-b7949574553c",
   "metadata": {},
   "source": [
    "影响一般是：  \n",
    "1、参数更新过大，模型不稳定。  \n",
    "2、损失函数值剧烈波动。  \n",
    "3、训练过程发散。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be800359-9dfa-4fc8-b66b-ec4a2a89c44e",
   "metadata": {},
   "source": [
    "**上述两种情况的解决方案：**  \n",
    "1、选用优秀的权重初始化方法。  \n",
    "2、选用合适的激活函数。   \n",
    "3、使用批归一化。  \n",
    "4、使用残差连接机制。  \n",
    "5、使用梯度裁剪方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b485a0d8-f42c-4598-816c-127c201ea344",
   "metadata": {},
   "source": [
    "**梯度检查点技术：（Gradient Checkpointing）**  \n",
    "“激活值”的概念：本质上就是每一层经过非线性变换后的输出。  \n",
    "梯度检查点概念：一种内存优化技术，在标准反向传播中，必须保存所有激活值来计算梯度，但检查点技术则是通过在前向传播过程中只保存部分关键位置的中间激活值（即“检查点”），而在反向传播过程中，当需要某个未保存的激活值时，从最近的检查点重新执行前向计算来恢复该值。这种方法以增加计算量为代价，显著减少了训练过程中的内存占用，实现了内存与计算开销之间的权衡。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415de4a5-da5c-44d7-8dca-7f27eb1e1787",
   "metadata": {},
   "source": [
    "工作流程：  \n",
    "1、前向传播：计算并保存检查点层的激活值丢弃非检查点层的激活值。   \n",
    "2、反向传播：从最近的检查点开始，重新计算到目标层的激活值。使用重新计算的激活值计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91398c12-13ed-4ced-b290-4effeb8c0c75",
   "metadata": {},
   "source": [
    "作用：  \n",
    "1、节省内存，减少GPU显存占用。  \n",
    "2、支持更大的模型。  \n",
    "3、权衡计算和内存，以增加计算时间的代价节省内存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c99105-cb71-4af1-a918-d601a7898b6f",
   "metadata": {},
   "source": [
    "**梯度累积技术：（Gradient Accumulation）**  \n",
    "概念：梯度累积是一种模拟大批量训练的技术，通过多次小批量计算梯度并累积，最后一次性更新参数。    \n",
    "原理：在标准的小批量训练中，每次前向传播和反向传播后立即更新参数。而在梯度累积中，多次小批量的梯度被累积起来，等累积到指定次数后再更新参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13479d95-4c83-4683-bc25-7ab2b180ed2a",
   "metadata": {},
   "source": [
    "更新公式：  \n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t-\\frac{\\mu}{n}\\sum_{i=1}^n \\nabla_{\\theta}J_i(\\theta_t)\n",
    "$$  \n",
    "其中n是累计步数，$J_i$是第i个小批量的损失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b897009-dc7c-4768-8dee-cfe20a449278",
   "metadata": {},
   "source": [
    "解决的问题：  \n",
    "1、显存不足：当GPU显存无法支持大批量训练时，通过梯度累积模拟大批量效果。  \n",
    "2、训练稳定性：大批量训练通常更稳定，梯度累积可以实现类似效果。  \n",
    "3、泛化性能：大批量训练有助于提高模型的泛化能力。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c3e0d8-eb76-479b-b077-cb4f1052c2fb",
   "metadata": {},
   "source": [
    "**梯度裁剪技术：（Gradient Clipping）**  \n",
    "梯度裁剪是⼀种防止**梯度爆炸的技术**，通过限制梯度的范数来稳定训练过程。当梯度的范数超过设定阈值时，按比例缩小梯度，使其不超过阈值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebb7b0-f1bc-4aad-b021-2a073faae901",
   "metadata": {},
   "source": [
    "法一：按范数裁剪  \n",
    "核心思想：  \n",
    "将整个模型的所有梯度看作一个巨大的向量，计算这个向量的“总长度”（即范数），如果这个“总长度”超过了预设的阈值（threshold），就按比例缩小整个梯度向量，使其“长度”刚好等于阈值。   \n",
    "优点：  \n",
    "保持梯度方向：裁剪后，梯度向量的方向不变，只是长度被缩短。这保留了梯度提供的“学习信号”的相对关系。  \n",
    "全局控制：从整体上控制了梯度的“能量”，非常稳定  \n",
    "计算步骤：  \n",
    "1、将模型中所有可训练参数的梯度拼接成一个长向量g，然后计算其L2范数，$||g||_2$表示向量g的L2范数。\n",
    "$$\n",
    "{||g||}_2=\\sqrt{\\sum_i g_i^2}\n",
    "$$  \n",
    "其中$g_i$是第i个参数的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e40ad0-bce3-40aa-b814-23e3126d52f8",
   "metadata": {},
   "source": [
    "2、比较并裁剪.  \n",
    "如果$||g||$<=threshold，那就不进行操作。  \n",
    "如果$||g||$>threshold，则对所有梯度进行等比例缩放。\n",
    "$$\n",
    "g=g \\cdot \\frac{threshold}{||g||_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5a088d-ca04-4c3f-a2e2-7e1e94041347",
   "metadata": {},
   "source": [
    "法二：按值裁剪\n",
    "核心思想：对每一个单独的梯度值进行限制。如果某个梯度的值超过了预设的上限，就把它设为上限值；如果低于预设的下限，就把它设为下限值。  \n",
    "优点：  \n",
    "简单直观：实现容易，逻辑清晰。  \n",
    "局部控制：可以防止个别梯度值过大。  \n",
    "缺点：  \n",
    "破坏梯度方向：它粗暴地截断了极端值，可能会改变梯度向量的整体方向，丢失部分学习信号。  \n",
    "不够智能：即使只有一个梯度非常大，其他梯度正常，它也只修改那个大的值，而按范数裁剪会整体缩放，更均衡。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3213620-baad-4973-bdbb-838727054879",
   "metadata": {},
   "source": [
    "## 二、优化器及其相关问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d5950-3627-440a-86e9-9a9940431502",
   "metadata": {},
   "source": [
    "**优化器的作用：**  \n",
    "优化器是深度学习训练中的核⼼组件，负责根据梯度信息更新模型参数。其主要作用包括：  \n",
    "1、参数更新：根据梯度信息调整模型参数。  \n",
    "2、学习率调整：动态调整学习率以提高训练效果。  \n",
    "3、动量控制：利用历史梯度信息加速收敛。    \n",
    "4、自适应调整：根据不同参数的特点进行个性化调整。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34abe11-33d0-4e4b-b115-fcb5ad160e41",
   "metadata": {},
   "source": [
    "**常见优化器的类型：**  \n",
    "1、⼀阶优化器：基于梯度信息进行参数更新。    \n",
    "2、⼆阶优化器：基于梯度和⼆阶导数信息进行参数更新。    \n",
    "3、自适应优化器：自动调整学习率和其他超参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2bdd6f-81c5-4787-9a85-e368481ef2e6",
   "metadata": {},
   "source": [
    "**SGD优化器：（随机梯度下降）**  \n",
    "适用场景：适用于对泛化性能要求较高的任务，或计算资源有限的场景。  \n",
    "基本公式：  \n",
    "$\\theta_{t+1}=\\theta_t-\\eta\\nabla_{\\theta}J(\\theta_t)$  \n",
    "优点：  \n",
    "1、简单高效，泛化能力好。\n",
    "2、理论基础扎实，数学收敛性分析成熟。\n",
    "3、适合大规模数据。  \n",
    "缺点：  \n",
    "1、收敛速度慢，训练时间长。  \n",
    "2、对学习率敏感：学习率太大 → 损失震荡，甚至发散。学习率太小 → 收敛太慢，陷入局部极小值。   \n",
    "3、路径震荡严重（无动量时）  \n",
    "4、容易陷入局部极小值。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedda3c-dd3d-4533-a757-ee6909fcabd1",
   "metadata": {},
   "source": [
    "**Momentum优化器：**  \n",
    "基本公式：  \n",
    "$v_{t+1}=\\gamma v_t+\\eta\\nabla_{\\theta}J(\\theta_t)$  \n",
    "$\\theta_{t+1}=\\theta_t-v_{t+1}$  \n",
    "其中$v_t$是动量项，$\\gamma$是动量系数，通常取0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cd254-526f-4a2e-ac96-4ad040c00922",
   "metadata": {},
   "source": [
    "原理：引入动量项，积累历史梯度信息，使得参数更新具有惯性，能够加速收敛并减少震荡  \n",
    "作用：  \n",
    "1、加速收敛：在梯度方向一致的时候加速更新。  \n",
    "2、减少震荡：在梯度方向变化的时候减少震荡。  \n",
    "3、跳出局部最优：借助惯性有可能跳出局部最优。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21099cd6-64d3-4912-8225-f56eb2dd8bd2",
   "metadata": {},
   "source": [
    "**RMSProp优化器：**  \n",
    "基本公式：  \n",
    "$s_{t+1}=\\gamma s_t+(1-\\gamma)[\\nabla_{\\theta}J(\\theta_t)]^2$  \n",
    "$\\theta_{t+1}=\\theta_t-\\frac{\\eta}{\\sqrt{s_{t+1}+\\epsilon}}\\nabla_{\\theta}J(\\theta_t)$  \n",
    "其中,$s_t$是梯度平方的指数移动平均，$\\epsilon$是平滑项（防止除零）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e7362-b820-49f3-b176-42d442b30016",
   "metadata": {},
   "source": [
    "原理：自适应调整学习率，对不同参数使用不同的学习率。梯度大的参数使用较小的学习率，梯度小的参数使\n",
    "用较大的学习率。  \n",
    "作用：  \n",
    "1、自适应学习率：根据历史梯度信息自动调整学习率  \n",
    "2、稳定训练：减少训练过程中的波动  \n",
    "3、快速收敛：在非凸优化问题中表现良好  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04730a85-9c8f-418f-9acd-d9292ac08ad4",
   "metadata": {},
   "source": [
    "**Adam优化器：（结合了RMSProp和Momentum的优点）**  \n",
    "适用场景：适用于大多数深度学习任务，特别是需要快速收敛的场景。  \n",
    "基本公式：  \n",
    "$m_{t+1}=\\beta_1 m_t+(1-\\beta_1)\\nabla_{\\theta}J(\\theta_t) $  \n",
    "$s_{t+1}=\\beta_2 s_t +(1-\\beta_2)[\\nabla_{\\theta}J(\\theta_t)]^2$  \n",
    "$\\hat{m_{t+1}}=\\frac{m_{t+1}}{1-\\beta_1^t}$     \n",
    "$\\hat{s_{t+1}}=\\frac{s_{t+1}}{1-\\beta_2^t}$  \n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t-\\frac{\\eta}{\\sqrt{\\hat{s_{t+1}}+\\epsilon}}\\hat{m_{t+1}}\n",
    "$$\n",
    "其中，$m_t$是梯度的⼀阶矩估计，$s_t$是梯度的⼆阶矩估计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d4005-db98-4885-94d9-6afc77201a3a",
   "metadata": {},
   "source": [
    "原理：考虑了梯度的历史信息，又对不同参数使用自适应学习率。  \n",
    "作用：  \n",
    "1、动量加速：利用历史梯度信息加速收敛   \n",
    "2、自适应学习率：对不同参数使用不同的学习率\n",
    "3、偏差修正：通过偏差修正项提高初期估计的准确性  \n",
    "常用参数：  \n",
    "$\\beta_1=0.9$（一阶矩估计的衰减率）  \n",
    "$\\beta_2=0.999$（二阶矩估计的衰减率）  \n",
    "$\\epsilon=10^{-8}$（平滑项）  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7392a-ba0c-486d-829b-1b118f232473",
   "metadata": {},
   "source": [
    "**AdamW优化器：**  \n",
    "在Adam的基础上改进了权重衰减的处理方式。  \n",
    "基本公式：  \n",
    "$m_{t+1}=\\beta_1 m_t+(1-\\beta_1)\\nabla_{\\theta}J(\\theta_t) $  \n",
    "$s_{t+1}=\\beta_2 s_t +(1-\\beta_2)[\\nabla_{\\theta}J(\\theta_t)]^2$  \n",
    "$\\hat{m_{t+1}}=\\frac{m_{t+1}}{1-\\beta_1^t}$     \n",
    "$\\hat{s_{t+1}}=\\frac{s_{t+1}}{1-\\beta_2^t}$  \n",
    "$$\n",
    "\\theta_{t+1}=\\theta_t-\\eta(\\frac{\\hat{m_{t+1}}}{\\sqrt{\\hat{s_{t+1}}+\\epsilon}}+\\lambda \\theta_t)\n",
    "$$  \n",
    "其中，$\\lambda$是权重衰减系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2cfc78-0717-4cc3-900c-4d0cb1f68352",
   "metadata": {},
   "source": [
    "优化点：将权重衰减与梯度更新分离，直接在参数更新时加入权重衰减项，而不是在损失函数中加入L2正则化项。在参数更新时同时考虑了梯度信息和权重衰减，有助于提高模型的泛化性能。  \n",
    "优势：  \n",
    "1、更准确的权重衰减：避免了Adam中权重衰减与L2正则化等价性的误解。  \n",
    "2、更好的泛化性能：在许多任务上表现优于Adam。  \n",
    "3、更稳定的训练：减少过拟合风险。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2383bd2-ad5b-4a6d-8efb-a815635318d6",
   "metadata": {},
   "source": [
    "**Muon优化器：**  \n",
    "概念：Muon是⼀种专门为大预言模型设计的优化器，结合了Adam和SGD的优点，在训练大模型时表现出色。  \n",
    "核心思想：  \n",
    "1、对重要部分参数使用Adam优化器来加速收敛。  \n",
    "2、对其他不那么重要参数使用SGD优化器来减少内存占用。  \n",
    "3、通过智能分配策略决定哪些参数使用哪种优化器。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc52fd3f-2f8a-4af9-b2ea-b77a4645b3e3",
   "metadata": {},
   "source": [
    "优势：    \n",
    "1、高效训练：在大模型训练中比纯Adam或纯SGD更高效。    \n",
    "2、内存友好：相比Adam减少内存占用。   \n",
    "3、收敛稳定：结合了两种优化器的优点。  \n",
    "4、计算效率：减少了需要存储的优化器状态数量，降低了计算开销。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef37ce38-4079-4892-b9f4-4df3284fe2d8",
   "metadata": {},
   "source": [
    "缺点：  \n",
    "Muon有训练崩溃问题。  \n",
    "1、梯度方差大：对于某些参数组，梯度的方差可能非常大，导致参数更新剧烈波动。  \n",
    "2、自适应机制失效：在某些情况下，Muon的自适应机制可能无法有效控制梯度更新。  \n",
    "3、混合优化的不协调：不同优化策略之间的切换可能导致训练过程不稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa0337-2dc3-4c81-94b5-657f2b866902",
   "metadata": {},
   "source": [
    "**Loss缩放和学习率调整上的区别：**  \n",
    "结论：对于SGD这种基础单阶优化器，没有区别。但对于Adam等高阶优化器就有区别。  \n",
    "为什么会有区别？  \n",
    "见教案，提示：可以从推导公式上求解。  \n",
    "在Adam优化器中，Loss缩放和学习率调整会产生完全不同的优化路径，这是因为Loss缩放改变了梯度的幅度，进而影响了自适应学习率的计算，而学习率调整只改变了最终的更新步长而不影响自适应学习率的计算过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a458c1-b566-433a-b195-1d826978788d",
   "metadata": {},
   "source": [
    "**Kimi-K2的Muon-clip：**  \n",
    "提出背景：    \n",
    "1、稳定性需求：在大规模模型训练中，稳定性比速度更重要。  \n",
    "2、实际应用挑战：在实际训练中发现，纯Muon优化器在某些阶段会出现梯度爆炸现象。  \n",
    "3、优化策略改进：需要在保持Muon优势的同时，增强训练的鲁棒性。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e13369-5ee8-43f4-b686-72da63117e64",
   "metadata": {},
   "source": [
    "原理：  \n",
    "结合了Muon优化器和梯度裁剪的思想。  \n",
    "1、参数分组：  \n",
    "将模型参数分为不同的组，对不同组的参数采用不同的优化策略。  \n",
    "2、自适应裁剪：  \n",
    "根据参数的重要性动态调整梯度裁剪阈值，对重要参数使用较小的裁剪阈值，对非重要参数使用较大的裁剪阈值。  \n",
    "3、混合优化：  \n",
    "对关键参数使用Adam优化器以保证收敛速度对其他参数使用SGD优化器以提高泛化性能。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8437b2-91c7-4497-9a28-b659896ef938",
   "metadata": {},
   "source": [
    "Kimi解决崩溃问题的方：  \n",
    "1、梯度范数控制：  \n",
    "实时监控梯度范数，防止梯度爆炸。对异常大的梯度进行裁剪，保持更新的稳定性。  \n",
    "2、动态阈值调整：  \n",
    "根据训练阶段动态调整裁剪阈值。在训练初期使用较宽松的阈值，后期使用较严格的阈值。  \n",
    "3、分层保护机制：  \n",
    "对不同层的参数采用不同的保护策略。关键层使用更严格的保护措施。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c1ab3-69f1-4cac-85d0-d794b401df08",
   "metadata": {},
   "source": [
    "优势：  \n",
    "1、高效训练：结合了不同优化器的优点。  \n",
    "2、稳定收敛：通过自适应裁剪保证训练稳定性。  \n",
    "3、资源优化：针对不同参数采用最适合的优化策略。  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
