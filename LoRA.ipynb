{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf39584-6e0b-4754-b4a8-94427b034639",
   "metadata": {},
   "source": [
    "# LoRA低秩适配学习笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944bfe0c-68b8-4ecf-b684-097f0a4290b1",
   "metadata": {},
   "source": [
    "## 一、了解LoRA（Low-Rank Adaptation）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714e9368-5734-4128-abac-8775df57a1f7",
   "metadata": {},
   "source": [
    "### （一）LoRA的定义： \n",
    "LoRA是一种参数高效的微调方法，通过在预训练矩阵的权重中引入低秩分解来实现参数高效的微调。  \n",
    "LoRA的核心思想是：大型预训练模型的权重更新有低秩特性，因此可以用低秩矩阵来近似表示权重变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c655b-0a0d-40de-b4df-163496fc4aa1",
   "metadata": {},
   "source": [
    "### （二）LoRA的基本原理：  \n",
    "**核心假设:** 预训练模型在特定任务上的权重更新可以近似为低秩矩阵，即权重更新矩阵$\\Delta W$可以分解为两个小矩阵的乘积。  \n",
    "$$\\Delta W=B\\times A$$   \n",
    "其中:  \n",
    "$A\\in R^{r×k}$  \n",
    "$B\\in R^{d×r}$  \n",
    "$ r≪min(d,k)$ r是秩参数。  \n",
    "**在标准线性变换中：**  \n",
    "$$h=W_0x$$\n",
    "**使用LoRA后：**\n",
    "$$h=(W_0+\\Delta W)x=(W_0+B\\times A)x=W_0x+BAx$$  \n",
    "其中$W_0$是冻结的预训练权重矩阵，B和A是可训练的低秩矩阵，$\\Delta W$是权重更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37114d1-f594-4d52-818d-565e008eab6f",
   "metadata": {},
   "source": [
    "### （三）LoRA工作流程：   \n",
    "1. 冻结预训练权重：保持原始权重矩阵$W_0$不变  \n",
    "2. 引入低秩矩阵：添加可训练的低秩矩阵$B$和$A $ \n",
    "3. 前向传播：在前向传播时计算$W_0+B\\times A$  \n",
    "4. 参数更新：只优化$B$和$A$矩阵  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f372f4-5ae7-472d-a8f5-b703e4f4061b",
   "metadata": {},
   "source": [
    "### （四）为什么LoRA有效：\n",
    "**理论基础：**  \n",
    "1、低秩假设的合理性：研究表明，预训练模型在微调过程中权重的更新往往具有低秩特性，这意味着大部分更新信息可以被少数几个主要方向捕获。  \n",
    "2、神经网络的过参数化：大型神经网络通常具有过参数化的特性，许多参数之间存在冗余，因此可以用更少的参数来近似表示权重更新。  \n",
    "\n",
    "**有效性原理分析：**  \n",
    "1、保持预训练知识：原始权重矩阵保持不变，避免灾难性遗忘问题，保留了预训练模型的通用功能。  \n",
    "2、高效的参数利用：低秩矩阵能够捕获主要的更新方向，减少了参数冗余，提升了参数利用效率。  \n",
    "3、良好的泛化能力：低秩约束有了正则化的作用，避免过拟合，提升模型的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf86ea-66e4-43c5-923d-00b0707d6339",
   "metadata": {},
   "source": [
    "## 二、LoRA矩阵的初始化方法和原理：  \n",
    "### （一）对矩阵B和A的初始化： \n",
    "\n",
    "矩阵B使用0矩阵初始化 **（升维矩阵）**  $B=0$  \n",
    "矩阵A使用高斯分布初始化 **（降维矩阵）**  $A=N~(0,1)$   \n",
    "$A=N~(0,\\frac{\\sigma^2}{r})$ （其中$\\sigma^2$是标准方差通常为1，r是秩，这么做的原因是 **控制低秩更新的整体尺度，使其不依赖于r的大小**，这其实是将原本要$\\Delta W=\\frac{\\alpha}{r}BA$的那个缩放参数吸收进了初始化，使外面得以保持$\\Delta W=BA$）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c98f629-a340-41b5-8dcd-e9e7e157b24d",
   "metadata": {},
   "source": [
    "### （二）对初始化的考虑：   \n",
    "\n",
    "**对B矩阵要初始化为0的原因：**  \n",
    "1. 初始无影响：初始时$B\\times A=0$不影响原始矩阵。\n",
    "2. 渐进学习：模型可以从0开始逐步学习适配。\n",
    "3. 稳定性：避免了初始损失和梯度剧烈震荡。\n",
    "\n",
    "**为什么不能对A矩阵进行初始化为0：**   \n",
    "1. 梯度消失：初始梯度为0，无法有效更新。\n",
    "2. 对称性问题：所有神经元初始相同，无法有效分化。\n",
    "3. 学习困难：模型难以从0开始学习有效表示。\n",
    "\n",
    "**为什么对A矩阵使用高斯初始化：**\n",
    "1. 打破对称性：不同的初始值帮助神经元学习不同的特征。  \n",
    "2. 提供初始信号：为后续层提供非0的输入信息。  \n",
    "3. 梯度流动：确保反向传播时有有效的梯度流。  \n",
    "\n",
    "**为什么二者不可以同时为0：**    \n",
    "用运算分析，见教案  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947bed40-a36d-4a63-88d4-972ab894d08f",
   "metadata": {},
   "source": [
    "### （三）变换初始化方法的后果考虑  \n",
    "\n",
    "**升维矩阵用高斯初始化，降维矩阵用0初始化：**   \n",
    "1. 初始影响：即使降维矩阵为零，升维矩阵的非零值仍可能产生影响。  \n",
    "2. 梯度问题：降维矩阵为零可能导致梯度流动不畅。   \n",
    "3. 学习效率：可能需要更长时间才能学习到有效的适配。\n",
    "\n",
    "**都使用0初始化：**  \n",
    "1. 梯度消失：初始时所有梯度为零，无法开始学习。  \n",
    "2. 对称性问题：所有参数初始相同，无法有效分化。  \n",
    "3. 学习失败：模型可能无法从零开始学习。\n",
    "\n",
    "**都使用高斯初始化：**  \n",
    "1. 初始干扰：LoRA在初始时就会对模型产生显著影响。  \n",
    "2. 稳定性问题：可能导致训练初期的不稳定。  \n",
    "3. 预训练知识破坏：可能干扰预训练模型的已有知识。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43efe3a3-195e-4238-a8bd-8e515fbc548d",
   "metadata": {},
   "source": [
    "## 三、为什么LoRA使用两个矩阵相乘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4f9af-1edf-4e81-b635-3d0e3430e88b",
   "metadata": {},
   "source": [
    "### （一）低秩分解的必要性  \n",
    "使用两个矩阵相乘的原因在于效率，假设原始维度为$d\\times k$:  \n",
    "一个矩阵需要参数：$dk$  \n",
    "两个矩阵需要参数：$dr+rk$ （r远小于d,k）  \n",
    "参数量显著减少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e2244e-973a-4236-87c2-81405f5c2508",
   "metadata": {},
   "source": [
    "### （二）不能只要一个矩阵的原因  \n",
    "1、参数效率的损失：如果只用一个矩阵，就无法实现参数的显著减少。    \n",
    "2、表达能力的限制：单个矩阵可能无法有效捕获权重更新的主要方向。  \n",
    "3、低秩约束：两个矩阵的乘积天然具有低秩特性，符合LoRA的核心假设。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23390bb-6e60-45cd-b2c9-bbe28bbbdecd",
   "metadata": {},
   "source": [
    "### （三）低秩近似的有效性  \n",
    "根据矩阵分析理论，许多实际矩阵都可以用低秩近似来有效表示，特别是权重更新矩阵。  \n",
    "在多个NLP任务中，LoRA使用较小的rank值（如4、8、16）就能达到很好的效果，证明了低秩近似的有效性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c19130-7855-4805-a0a9-65d2ed5da74a",
   "metadata": {},
   "source": [
    "## 四、LoRA微调的层的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64338e5-9743-4bd1-952e-4a4063124051",
   "metadata": {},
   "source": [
    "### 注意力层LoRA  \n",
    "作用：  \n",
    "1、控制注意力机制的行为  \n",
    "2、调整模型关注的信息  \n",
    "3、影响序列中不同位置的交互  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c095e5-45a7-4a8f-83c5-a99b60cbed43",
   "metadata": {},
   "source": [
    "### 前馈网络层LoRA  \n",
    "作用：  \n",
    "1、控制特征变换过程    \n",
    "2、调整非线性变换行为  \n",
    "3、影响模型的表达能力  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dfa9e-3398-4bc6-89f9-7a3e2dc349e1",
   "metadata": {},
   "source": [
    "## 五、LoRA中Rank和Alpha参数    \n",
    "Alpha参数：alpha 是 LoRA 的“控制旋钮”，用来调节低秩更新（ΔW）对原始模型的影响强度。 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ba781-4a3d-4d64-a711-4f08be3989fc",
   "metadata": {},
   "source": [
    "### （一）Rank对微调效果的影响  \n",
    "1、过小：表达能力不足，可能欠拟合。  \n",
    "2、过大：参数过多，可能过拟合。    \n",
    "3、适中：平衡表达能力和泛化性能。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf37625b-a5d5-44d2-8f05-671d724a3f4e",
   "metadata": {},
   "source": [
    "### （二）Alpha对微调效果的影响  \n",
    "1、过小：适配强度不足，效果不明显。  \n",
    "2、过大：过度改变原模型，可能破坏预训练知识。  \n",
    "3、适中：有效适配，保持预训练优势。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f9111-a06f-4cf0-ae9e-19160c877b9d",
   "metadata": {},
   "source": [
    "## 六、LoRA的变体  \n",
    "### （一）LoRA+  \n",
    "**实现原理：**  \n",
    "矩阵B和A的梯度范数（梯度向量的欧几里得长度，越大越陡）有显著差异，可以设置不同的学习率。  \n",
    "具体而言：  \n",
    "A的梯度范数通常较小  \n",
    "B的梯度范数通常较大  \n",
    "因此：  \n",
    "对矩阵A使用高学习率$\\mu_A=\\alpha_A\\times \\mu_0$  \n",
    "对矩阵B使用低学习率$\\mu_B=\\alpha_B\\times \\mu_0$  \n",
    "其中$\\mu_0$是初始学习率，$\\alpha_A$是矩阵A的学习率倍数  \n",
    "这样设置的原因：  \n",
    "矩阵A需要更快的学习速度来快速适应任务  \n",
    "矩阵B需要较慢的学习速度来保持稳定性  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc8d25-5501-4b52-a2b3-4db1fccb6224",
   "metadata": {},
   "source": [
    "### （二）AdaLoRA(Adaptive LoRA) \n",
    "**核心思想：**  \n",
    "通过奇异值分解（SVD）来动态调整LoRA的秩，实现更精细的参数控制，根据参数重要性自动调整每一个LoRA模块的秩。  \n",
    "**优化点：**  \n",
    "1、动态秩调整：根据重要性动态调整每个LoRA模块的秩。  \n",
    "2、参数重要性评估：通过奇异值评估参数重要性。   \n",
    "3、自适应剪枝：移除不重要的参数。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5dc90-2c96-412c-b7cb-d8fd3b6529ef",
   "metadata": {},
   "source": [
    "**数学基础：**  \n",
    "**奇异值分解**  \n",
    "定义：奇异值分解（SVD）就是将一个任意的实数或复数矩阵分解为三个特定矩阵的乘积。  \n",
    "具体做法，见教案\n",
    "**奇异值对于矩阵的重要性：**  \n",
    "奇异值反映了矩阵在各个主方向上的能量或重要性。较大的奇异值对应更重要的信息，较小的奇异值对应次要信息。  \n",
    "**重要性评估原理：**  \n",
    "奇异值的平方$\\sigma_i^2$代表了该方向上的方差或信息量。    \n",
    "累积奇异值的平方和代表了矩阵的总能量$\\sum_{r=i}^r \\sigma_i^2$  \n",
    "通过保留主要的奇异值，可以近似表示原矩阵的主要信息。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a17d3-b77c-4e77-8fad-af9ad6dd3644",
   "metadata": {},
   "source": [
    "**实现原理：**  \n",
    "在训练过程中定期进行SVD分析，根据奇异值的大小决定保留哪些参数。  \n",
    "具体步骤：  \n",
    "1、SVD分解：对LoRA矩阵的BA矩阵进行奇异值分解  \n",
    "$$W_{lora}=U\\Sigma V^T=\\sum_{i=1}^r\\sigma_i u_i v_i^T$$\n",
    "其中，$\\sigma_i$是降序排列后的第i个奇异值，$u_i$是左奇异向量$v_i$是右奇异向量。   \n",
    "2、重要性评估：根据奇异值大小评估参数重要性  \n",
    "$$Importance_i=\\sigma^2$$  \n",
    "3、秩调整：保留重要性高的奇异值对应的参数(保留前K个重要性大于阈值参数$\\epsilon$的方向)   \n",
    "$$\n",
    "r_{new}=arg\\min\\limits_k \\frac{\\sum_{i=k+1}^r \\sigma_i^2}{\\sum_{i=1}^r \\sigma_i^2}≤\\epsilon\n",
    "$$  \n",
    "其中，$\\epsilon$是阈值参数，表示可以接受的信息损失比例。  \n",
    "4、参数更新：重构LoRA矩阵继续训练  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995c8327-b7dc-406e-9681-954b8b7b4768",
   "metadata": {},
   "source": [
    "**自适应调整策略：**  \n",
    "AdaLoRA采用以下策略来决定何时该**增加**还是**减少**秩  \n",
    "1、性能监控：监控训练损失和验证损失，当验证损失停滞或增加时考虑调整秩。  \n",
    "2、重要性阈值：设定重要性阈值$\\epsilon$m，移除重要性低于阈值的奇异值对应的参数。  \n",
    "3、最小最大秩限制：防止秩降为0或无限增长。  \n",
    "4、控制调整频率：不是每一次迭代都调整，每固定步数调整一次。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca6532c-4440-4328-b616-310a4262c792",
   "metadata": {},
   "source": [
    "**优势：**     \n",
    "1、参数效率提升：自动移除不重要参数   \n",
    "2、性能保持：保持模型性能的同时减少参数   \n",
    "3、自适应性：根据任务需求自动调整  \n",
    "4、理论基础扎实：基于SVD的数学原理，有坚实的理论支撑  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8440f-b96c-41df-8780-d02fac1a5f7f",
   "metadata": {},
   "source": [
    "**实际应用的消耗考虑：**  \n",
    "1. SVD计算开销：SVD计算相对耗时，需要平衡计算开销和收益  \n",
    "2. 调整频率：不是每次迭代都进行调整，通常每隔一定步数进行一次  \n",
    "3. 最小秩限制：设置最小秩限制，避免秩降为0  \n",
    "4. 稳定性：确保调整过程不会破坏训练稳定性  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164bef1-a87b-4dfa-8d33-39a6096fa2bc",
   "metadata": {},
   "source": [
    "### （三）QLoRA  \n",
    "原理：在 LoRA 基础上引入4-bit 量化（如 NF4）和分页优化器（Paged Optimizers）。  \n",
    "改进：将预训练模型权重量化为 4-bit 存储，仅在前向传播时反量化为 16-bit；LoRA 适配器仍以 16-bit 训练。  \n",
    "效果：大幅降低显存占用（可实现 7B 模型在 24GB 显卡上微调），使大模型微调可在消费级设备上运行  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2077b4d-a5e9-49e3-84d3-6326590b60a9",
   "metadata": {},
   "source": [
    "### （四）DoRA  \n",
    "DoRA 不是直接学习权重更新，而是将预训练权重分解为“方向”和“幅度”两部分，然后只用低秩方式去调整“方向”，从而更高效地引导模型适应新任务  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d699495-b55c-49d1-a40a-7644b0299353",
   "metadata": {},
   "source": [
    "**DoRA分解原理：**  \n",
    "不同于传统LoRA，DoRA分解为幅度和方向  \n",
    "$$\n",
    "W=(m+\\Delta m)\\cdot \\frac{v+\\Delta v}{||v+\\Delta v||}\n",
    "$$  \n",
    "其中，原始权重是$W_0$，$v=W_0$是方向参数，$m=||v||$是其范数（幅度参数）      \n",
    "$\\Delta m$是一个可学习的低秩偏移  \n",
    "$\\Delta v$是对应的变量，且v会分解为低秩更新矩阵$\\Delta v=B_vA_v$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9498dd7-dd5b-4309-ba18-bbdf57a029cc",
   "metadata": {},
   "source": [
    "**效果：**  \n",
    "更精细地控制权重更新，解耦了方向与尺度变化，显著提升模型理解和泛化能力，尤其在复杂任务上优于标准 LoRA。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
