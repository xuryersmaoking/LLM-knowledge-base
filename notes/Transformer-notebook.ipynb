{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2f72f4-61cf-4a70-b553-b046a4e43d83",
   "metadata": {},
   "source": [
    "# Transformer疑问以及补充笔记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62377f42-5fd0-47d8-a229-8fc50e1b1acd",
   "metadata": {},
   "source": [
    "## 1、Transformer整体架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a056281-75b2-4427-91b7-fb3f7243e4ec",
   "metadata": {},
   "source": [
    "### 1.3、一句话是如何变成向量的："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7d5126-66d7-4e09-af8f-7407e51d451b",
   "metadata": {},
   "source": [
    "1、分词：将句子分成一个个词汇  \n",
    "2、数值化：对照自己构建的或用别人预训练的词汇表进行数值化，数值化后通常是一个整数列表  \n",
    "3、嵌入层转换：使用嵌入层将每一个ID转换为密集向量，输出二维张量（对于一个句子来说）。    \n",
    "&ensp;&ensp;嵌入层：一个强化版的词汇表，不仅记录有哪些词，还为这些词分配了一个**语义向量（推荐512维，性价比最高）**。  \n",
    "4、位置编码：为每一个词添加位置信息，transformer没有位置的概念，不添加位置信息，打乱句子顺序后的输出是一样的。一个词向量如果有512维，他的位置信息也有512维。10000是缩放常数，是用来控制正弦波和余弦波的频率范围。对于偶数维度使用sin位置编码，对于奇数维度使用cos位置编码。  \n",
    "    \n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{\\text{model}}}}}\\right)\n",
    "$$  \n",
    "5、嵌入与位置编码相加：将词汇表中每个词查表得到的 512维词嵌入向量，与对应位置通过公式计算出的 512维位置编码向量，进行向量加法，得到最终输入到 Transformer 的表示。  \n",
    "\n",
    "\n",
    "关于一个问题的解释：既然位置信息都已经加到了词向量中，模型又怎么得知位置的信息呢？  \n",
    "相加是一种优雅的隐式编码：它让位置信息**渗透**到每个维度，而不是作为一个外部信号。当一个词出现在不同位置时，它的**混合向量**会不同，导致它在上下文中的行为也不同。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9ecafa-1416-4f89-852a-92bc65253f8b",
   "metadata": {},
   "source": [
    "## 2、Transformer的q,k,v过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089b5f9-38f0-4668-a959-8f56a2cefc57",
   "metadata": {},
   "source": [
    "### 2.2、q,k,v过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eaba20-5f6c-4f75-89c5-288d2fb2e4bf",
   "metadata": {},
   "source": [
    "q代表查询，k代表键向量，与查询向量匹配，计算注意力分数，v代表值向量，代表实际内容。  \n",
    "通过嵌入层得到的向量表示x，并设置权重矩阵Wk,Wq,Wv，这三个矩阵维度是由嵌入维度和注意力头数决定。Wk,Wq,Wv的维度都是 d_model × d_model，但它们**内部**被分成了 num_heads 份，每份对应一个头，大小为 d_model × (d_model // num_heads)。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5043ee17-5f04-4f8f-b4ab-970396abd08b",
   "metadata": {},
   "source": [
    "**计算q,k,v：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d33ea1-26b7-4281-aec5-8276241aed0d",
   "metadata": {},
   "source": [
    "以x中的x1（第一个词的嵌入维度）举例：  \n",
    "$$ \n",
    "\\begin{aligned}\n",
    "Q_1 = x_1 \\cdot Wq \\\\  \n",
    "K_1 = x_1 \\cdot Wk \\\\ \n",
    "V_1 = x_1 \\cdot Wv  \n",
    "\\end{aligned}\n",
    "$$\n",
    "这些就是x1的q,k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4da68-fc16-4528-b1a3-b9847eb3ba18",
   "metadata": {},
   "source": [
    "**计算注意力分数:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cf14d-80bb-4938-a6cd-146b9c06b9d9",
   "metadata": {},
   "source": [
    "需要计算每一个词之间的注意力分数，包括和自己。  \n",
    "$$\n",
    "\\begin{aligned}\n",
    "score(Q_1,K_1)=Q_1 \\cdot K_1^\\top \\\\\n",
    "score(Q_1,K_2)=Q_1 \\cdot K_2^\\top \\\\\n",
    "score(Q_1,K_3)=Q_1 \\cdot K_3^\\top \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95309047-7949-4c40-9e3d-53c367e641d8",
   "metadata": {},
   "source": [
    "**进行缩放:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf403c2-27c0-44fd-a13d-6f1649095b4b",
   "metadata": {},
   "source": [
    "先明确一个值d_K:$$d_k = \\frac{d_{\\text{model}}}{\\text{num_heads}}$$  \n",
    "num_heads是注意力头数量。  \n",
    "缩放公式（**以第一个查询位置对自己的注意力分数为例，后面都是这样**）：$$\\text{scaled\\_score}(Q_1,K_1)=\\frac{score(Q_1,K_1)}{\\sqrt{d_k}}$$  \n",
    "缩放将对**整个注意力分数矩阵**进行缩放，注意力分数矩阵的每一行代表着一个“查询位置”（Query Position）对所有“键位置”（Key Positions）的注意力分数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626f7849-0524-4ec4-96b1-990a827f6a6f",
   "metadata": {},
   "source": [
    "**进行softmax操作:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0d6cb-5b25-48b4-aa0a-a84dd1d2c408",
   "metadata": {},
   "source": [
    "对所有缩放后的分数应用Softmax函数，得到**注意力权重**。  \n",
    "$$\n",
    "\\text{attention\\_weights}=softmax(\\text{scaled\\_score}(Q_1,K_1))$$  \n",
    "这个同样也是对**整个注意力分数矩阵**进行操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e490ce5-7998-4db4-b8dd-0b327af61cdf",
   "metadata": {},
   "source": [
    "**计算注意力输出：**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fa578-13e1-48ef-8282-aea85e79a840",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myshixunenvironment)",
   "language": "python",
   "name": "myshixunenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
